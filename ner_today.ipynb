{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import sklearn_crfsuite\n",
    "import re\n",
    "import nereval\n",
    "\n",
    "from estnltk import Text\n",
    "from estnltk.taggers import NerTagger\n",
    "from estnltk.taggers import WordLevelNerTagger\n",
    "from estnltk.converters import text_to_json\n",
    "from estnltk.converters import json_to_text\n",
    "from estnltk.layer_operations import flatten\n",
    "from sklearn.metrics import classification_report\n",
    "from estnltk.taggers import Retagger\n",
    "from estnltk.taggers import CompoundTokenTagger\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nervaluate import Evaluator\n",
    "\n",
    "nertagger = NerTagger()\n",
    "word_level_ner = WordLevelNerTagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding TokenSplitter to make an equal amount of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenSplitter( Retagger ):\n",
    "    \"\"\"Splits tokens into smaller tokens based on regular expression patterns.\"\"\" \n",
    "    conf_param = ['patterns', 'break_group_name']\n",
    "    \n",
    "    def __init__(self, patterns, break_group_name:str='end'):\n",
    "        # Set input/output layers\n",
    "        self.input_layers = ['tokens']\n",
    "        self.output_layer = 'tokens'\n",
    "        self.output_attributes = ()\n",
    "        # Set other configuration parameters\n",
    "        if not (isinstance(break_group_name, str) and len(break_group_name) > 0):\n",
    "            raise TypeError('(!) break_group_name should be a non-empty string.')\n",
    "        self.break_group_name = break_group_name\n",
    "        # Assert that all patterns are regular expressions in the valid format\n",
    "        if not isinstance(patterns, list):\n",
    "            raise TypeError('(!) patterns should be a list of compiled regular expressions.')\n",
    "        # TODO: we use an adhoc way to verify that patterns are regular expressions \n",
    "        #       because there seems to be no common way of doing it both in py35 \n",
    "        #       and py36\n",
    "        for pat in patterns:\n",
    "            # Check for the existence of methods/attributes\n",
    "            has_match   = callable(getattr(pat, \"match\", None))\n",
    "            has_search  = callable(getattr(pat, \"search\", None))\n",
    "            has_pattern = getattr(pat, \"pattern\", None) is not None\n",
    "            for (k,v) in (('method match()',has_match),\\\n",
    "                          ('method search()',has_search),\\\n",
    "                          ('attribute pattern',has_pattern)):\n",
    "                if v is False:\n",
    "                    raise TypeError('(!) Unexpected regex pattern: {!r} is missing {}.'.format(pat, k))\n",
    "            symbolic_groups = pat.groupindex\n",
    "            if self.break_group_name not in symbolic_groups.keys():\n",
    "                raise TypeError('(!) Pattern {!r} is missing symbolic group named {!r}.'.format(pat, self.break_group_name))\n",
    "        self.patterns = patterns\n",
    "\n",
    "    def _change_layer(self, text, layers, status):\n",
    "        # Get changeble layer\n",
    "        changeble_layer = layers[self.output_layer]\n",
    "        # Iterate over tokens\n",
    "        add_spans    = []\n",
    "        remove_spans = []\n",
    "        for span in changeble_layer:\n",
    "            token_str = text.text[span.start:span.end]\n",
    "            for pat in self.patterns:\n",
    "                m = pat.search(token_str)\n",
    "                if m:\n",
    "                    break_group_end = m.end( self.break_group_name )\n",
    "                    if break_group_end > -1 and \\\n",
    "                       break_group_end > 0  and \\\n",
    "                       break_group_end < len(token_str):\n",
    "                        # Make the split\n",
    "                        add_spans.append( (span.start, span.start+break_group_end) )\n",
    "                        add_spans.append( (span.start+break_group_end, span.end) )\n",
    "                        remove_spans.append( span )\n",
    "                        # Once a token has been split, then break and move on to \n",
    "                        # the next token ...\n",
    "                        break\n",
    "        if add_spans:\n",
    "            assert len(remove_spans) > 0\n",
    "            for old_span in remove_spans:\n",
    "                changeble_layer.remove_span( old_span )\n",
    "            for new_span in add_spans:\n",
    "                changeble_layer.add_annotation( new_span )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = TokenSplitter(patterns=[re.compile(r'(?P<end>[A-ZÕÄÖÜ]{1}\\w+)[A-ZÕÄÖÜ]{1}\\w+'),\\\n",
    "                                         re.compile(r'(?P<end>Piebenomme)metsawaht'),\\\n",
    "                                         re.compile(r'(?P<end>maa)peal'),\\\n",
    "                                         re.compile(r'(?P<end>reppi)käest'),\\\n",
    "                                         re.compile(r'(?P<end>Kiidjerwelt)J'),\\\n",
    "                                         re.compile(r'(?P<end>Ameljanow)Persitski'),\\\n",
    "                                         re.compile(r'(?P<end>mõistmas)Mihkel'),\\\n",
    "                                         re.compile(r'(?P<end>tema)Käkk'),\\\n",
    "                                         re.compile(r'(?P<end>Ahjawalla)liikmed'),\\\n",
    "                                         re.compile(r'(?P<end>kohtumees)A'),\\\n",
    "                                         re.compile(r'(?P<end>Pechmann)x'),\\\n",
    "                                         re.compile(r'(?P<end>pölli)Anni'),\\\n",
    "                                         re.compile(r'(?P<end>külla)Rauba'),\\\n",
    "                                         re.compile(r'(?P<end>kohtowannem)Jaak'),\\\n",
    "                                         re.compile(r'(?P<end>rannast)Leno'),\\\n",
    "                                         re.compile(r'(?P<end>wallast)Kiiwita'),\\\n",
    "                                         re.compile(r'(?P<end>wallas)Kristjan'),\\\n",
    "                                         re.compile(r'(?P<end>Pedoson)rahul'),\\\n",
    "                                         re.compile(r'(?P<end>pere)Jaan'),\\\n",
    "                                         re.compile(r'(?P<end>kohtu)poolest'),\\\n",
    "                                         re.compile(r'(?P<end>Kurrista)kaudo'),\\\n",
    "                                         re.compile(r'(?P<end>mölder)Gottlieb'),\\\n",
    "                                         re.compile(r'(?P<end>wöörmündri)Jaan'),\\\n",
    "                                         re.compile(r'(?P<end>Oinas)ja'),\\\n",
    "                                         re.compile(r'(?P<end>ette)Leenu'),\\\n",
    "                                         re.compile(r'(?P<end>Tommingas)peab'),\\\n",
    "                                         re.compile(r'(?P<end>wäljaja)Kotlep'),\\\n",
    "                                         re.compile(r'(?P<end>pea)A'),\\\n",
    "                                         re.compile(r'(?P<end>talumees)Nikolai')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in files from the distributed corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "\n",
    "with open('divided_corpus.txt', 'r', encoding = 'UTF-8') as f:\n",
    "    txt = f.readlines()\n",
    "\n",
    "for fileName in txt:\n",
    "    file, subdistribution = fileName.split(\":\")\n",
    "    files[file] = subdistribution.rstrip(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making `ner` and `wordner` layers from goldstandard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "removed_layers = ['sentences', 'morph_analysis', 'wordner', 'compound_tokens', 'ner', 'words', 'tokens']\n",
    "for file in {key: value for key, value in files.items() if value in ('1', '2', '3')}:\n",
    "    with open(find(file.replace(\".json\", \".txt\"), \"./vallakohtufailid/\"), 'r', encoding='UTF-8') as f:\n",
    "        text = Text(f.read())\n",
    "        if f == \"Tartu_V6nnu_Ahja_id3502_1882a.txt\":\n",
    "            text = text.replace('..', '. .')\n",
    "        \n",
    "        text = text.tag_layer(['tokens'])\n",
    "        token_splitter.retag(text)\n",
    "        CompoundTokenTagger(tag_initials = False, tag_abbreviations = False, tag_hyphenations = False).tag(text)\n",
    "        text.tag_layer('morph_analysis')\n",
    "        \n",
    "        nertagger.tag(text)\n",
    "        word_level_ner.tag(text)\n",
    "\n",
    "        text.add_layer(flatten(text['ner'], 'flat_ner'))\n",
    "        text.add_layer(flatten(text['wordner'], 'flat_wordner'))\n",
    "\n",
    "        for x in removed_layers:\n",
    "            text.pop_layer(x)\n",
    "        text_to_json(text, file=os.getcwd() + \"/vallakohtufailid_nertagger/\" + file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the f1-scores\n",
    "Layer `ner` against `gold_ner` and `wordner` against `gold_wordner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_not_working = ['J2rva_Tyri_V22tsa_id22177_1911a.json', \\\n",
    "                     'J2rva_Tyri_V22tsa_id18538_1894a.json', \\\n",
    "                     'Tartu_V6nnu_Ahja_id3502_1882a.json', \\\n",
    "                     'J2rva_Tyri_V22tsa_id22155_1911a.json', \\\n",
    "                     'Saare_Kihelkonna_Kotlandi_id18845_1865a.json', \\\n",
    "                     'P2rnu_Halliste_Abja_id257_1844a.json', \\\n",
    "                     'Saare_Kaarma_Loona_id7575_1899a.json', \\\n",
    "                     'Tartu_R6ngu_Aakre_id14648_1829a.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gold = []\n",
    "test = []\n",
    "gold_ner = []\n",
    "test_ner = []\n",
    "for file in sorted(os.listdir(\"./vallakohtufailid_nertagger/\")):\n",
    "    appendable_gold_ner = []\n",
    "    appendable_test_ner = []\n",
    "    if file.endswith(\".json\"):\n",
    "        if file in files_not_working:\n",
    "            continue\n",
    "        else:\n",
    "            with open(\"./vallakohtufailid_nertagger/\" + str(file), 'r', encoding='UTF-8') as f_test, \\\n",
    "                open(\"./vallakohtufailid_json_flat/\" + str(file), 'r', encoding='UTF-8') as f_gold:\n",
    "                    test_import = json_to_text(f_test.read())\n",
    "                    gold_import = json_to_text(f_gold.read())\n",
    "\n",
    "                    for i in range(len(gold_import['flat_gold_wordner'])):\n",
    "                        gold.append(gold_import['flat_gold_wordner'][i].nertag[0])\n",
    "                    for i in range(len(test_import['flat_wordner'])):\n",
    "                        test.append(test_import['flat_wordner'][i].nertag[0])\n",
    "                    \n",
    "                    for i in range(len(gold_import['flat_gold_ner'])):\n",
    "                        ner = gold_import['flat_gold_ner'][i]\n",
    "                        appendable_gold_ner.append({\"label\": ner.nertag[0], \"start\": int(ner.start), \"end\": int(ner.end)})\n",
    "                    for i in range(len(test_import['flat_ner'])):\n",
    "                        ner = test_import['flat_ner'][i]\n",
    "                        appendable_test_ner.append({\"label\": ner.nertag[0], \"start\": int(ner.start), \"end\": int(ner.end)})\n",
    "\n",
    "    gold_ner.append(appendable_gold_ner)\n",
    "    test_ner.append(appendable_test_ner)\n",
    "labels = set(gold) \n",
    "sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.96      0.97      0.97    133687\n",
      "       B-LOC       0.14      0.23      0.18       509\n",
      "       I-LOC       0.05      0.02      0.03       212\n",
      "   B-LOC_ORG       0.00      0.00      0.00      1190\n",
      "   I-LOC_ORG       0.00      0.00      0.00       699\n",
      "      B-MISC       0.00      0.00      0.00       133\n",
      "      I-MISC       0.00      0.00      0.00       392\n",
      "       B-ORG       0.03      0.30      0.06       195\n",
      "       I-ORG       0.05      0.21      0.08       462\n",
      "       B-PER       0.79      0.74      0.76     11088\n",
      "       I-PER       0.85      0.68      0.75     10386\n",
      "\n",
      "    accuracy                           0.92    158953\n",
      "   macro avg       0.26      0.29      0.26    158953\n",
      "weighted avg       0.92      0.92      0.92    158953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gold, test, labels=sorted_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(gold_ner, test_ner, tags=['ORG', 'PER', 'MISC', 'LOC', 'LOC_ORG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, results_per_tag = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ent_type': {'correct': 8814,\n",
       "  'incorrect': 2056,\n",
       "  'partial': 0,\n",
       "  'missed': 2245,\n",
       "  'spurious': 2044,\n",
       "  'possible': 13115,\n",
       "  'actual': 12914,\n",
       "  'precision': 0.6825150998915905,\n",
       "  'recall': 0.6720548989706443,\n",
       "  'f1': 0.6772446117791694},\n",
       " 'partial': {'correct': 8114,\n",
       "  'incorrect': 0,\n",
       "  'partial': 2756,\n",
       "  'missed': 2245,\n",
       "  'spurious': 2044,\n",
       "  'possible': 13115,\n",
       "  'actual': 12914,\n",
       "  'precision': 0.7350162614217128,\n",
       "  'recall': 0.7237514296606938,\n",
       "  'f1': 0.7293403511467976},\n",
       " 'strict': {'correct': 7420,\n",
       "  'incorrect': 3450,\n",
       "  'partial': 0,\n",
       "  'missed': 2245,\n",
       "  'spurious': 2044,\n",
       "  'possible': 13115,\n",
       "  'actual': 12914,\n",
       "  'precision': 0.5745702338547313,\n",
       "  'recall': 0.5657643919176516,\n",
       "  'f1': 0.570133312843367},\n",
       " 'exact': {'correct': 8114,\n",
       "  'incorrect': 2756,\n",
       "  'partial': 0,\n",
       "  'missed': 2245,\n",
       "  'spurious': 2044,\n",
       "  'possible': 13115,\n",
       "  'actual': 12914,\n",
       "  'precision': 0.6283103608486913,\n",
       "  'recall': 0.61868089973313,\n",
       "  'f1': 0.6234584501901724}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

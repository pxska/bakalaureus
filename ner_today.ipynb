{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import sklearn_crfsuite\n",
    "import re\n",
    "\n",
    "from estnltk import Text\n",
    "from estnltk.taggers import NerTagger\n",
    "from estnltk.taggers import WordLevelNerTagger\n",
    "from estnltk.converters import text_to_json\n",
    "from estnltk.converters import json_to_text\n",
    "from estnltk.layer_operations import flatten\n",
    "from sklearn.metrics import classification_report\n",
    "from estnltk.taggers import Retagger\n",
    "from estnltk.taggers import CompoundTokenTagger\n",
    "\n",
    "\n",
    "nertagger = NerTagger()\n",
    "word_level_ner = WordLevelNerTagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding TokenSplitter to make an equal amount of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenSplitter( Retagger ):\n",
    "    \"\"\"Splits tokens into smaller tokens based on regular expression patterns.\"\"\" \n",
    "    conf_param = ['patterns', 'break_group_name']\n",
    "    \n",
    "    def __init__(self, patterns, break_group_name:str='end'):\n",
    "        # Set input/output layers\n",
    "        self.input_layers = ['tokens']\n",
    "        self.output_layer = 'tokens'\n",
    "        self.output_attributes = ()\n",
    "        # Set other configuration parameters\n",
    "        if not (isinstance(break_group_name, str) and len(break_group_name) > 0):\n",
    "            raise TypeError('(!) break_group_name should be a non-empty string.')\n",
    "        self.break_group_name = break_group_name\n",
    "        # Assert that all patterns are regular expressions in the valid format\n",
    "        if not isinstance(patterns, list):\n",
    "            raise TypeError('(!) patterns should be a list of compiled regular expressions.')\n",
    "        # TODO: we use an adhoc way to verify that patterns are regular expressions \n",
    "        #       because there seems to be no common way of doing it both in py35 \n",
    "        #       and py36\n",
    "        for pat in patterns:\n",
    "            # Check for the existence of methods/attributes\n",
    "            has_match   = callable(getattr(pat, \"match\", None))\n",
    "            has_search  = callable(getattr(pat, \"search\", None))\n",
    "            has_pattern = getattr(pat, \"pattern\", None) is not None\n",
    "            for (k,v) in (('method match()',has_match),\\\n",
    "                          ('method search()',has_search),\\\n",
    "                          ('attribute pattern',has_pattern)):\n",
    "                if v is False:\n",
    "                    raise TypeError('(!) Unexpected regex pattern: {!r} is missing {}.'.format(pat, k))\n",
    "            symbolic_groups = pat.groupindex\n",
    "            if self.break_group_name not in symbolic_groups.keys():\n",
    "                raise TypeError('(!) Pattern {!r} is missing symbolic group named {!r}.'.format(pat, self.break_group_name))\n",
    "        self.patterns = patterns\n",
    "\n",
    "    def _change_layer(self, text, layers, status):\n",
    "        # Get changeble layer\n",
    "        changeble_layer = layers[self.output_layer]\n",
    "        # Iterate over tokens\n",
    "        add_spans    = []\n",
    "        remove_spans = []\n",
    "        for span in changeble_layer:\n",
    "            token_str = text.text[span.start:span.end]\n",
    "            for pat in self.patterns:\n",
    "                m = pat.search(token_str)\n",
    "                if m:\n",
    "                    break_group_end = m.end( self.break_group_name )\n",
    "                    if break_group_end > -1 and \\\n",
    "                       break_group_end > 0  and \\\n",
    "                       break_group_end < len(token_str):\n",
    "                        # Make the split\n",
    "                        add_spans.append( (span.start, span.start+break_group_end) )\n",
    "                        add_spans.append( (span.start+break_group_end, span.end) )\n",
    "                        remove_spans.append( span )\n",
    "                        # Once a token has been split, then break and move on to \n",
    "                        # the next token ...\n",
    "                        break\n",
    "        if add_spans:\n",
    "            assert len(remove_spans) > 0\n",
    "            for old_span in remove_spans:\n",
    "                changeble_layer.remove_span( old_span )\n",
    "            for new_span in add_spans:\n",
    "                changeble_layer.add_annotation( new_span )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = TokenSplitter(patterns=[re.compile(r'(?P<end>[A-ZÕÄÖÜ]{1}\\w+)[A-ZÕÄÖÜ]{1}\\w+'),\\\n",
    "                                         re.compile(r'(?P<end>Piebenomme)metsawaht'),\\\n",
    "                                         re.compile(r'(?P<end>maa)peal'),\\\n",
    "                                         re.compile(r'(?P<end>reppi)käest'),\\\n",
    "                                         re.compile(r'(?P<end>Kiidjerwelt)J'),\\\n",
    "                                         re.compile(r'(?P<end>Ameljanow)Persitski'),\\\n",
    "                                         re.compile(r'(?P<end>mõistmas)Mihkel'),\\\n",
    "                                         re.compile(r'(?P<end>tema)Käkk'),\\\n",
    "                                         re.compile(r'(?P<end>Ahjawalla)liikmed'),\\\n",
    "                                         re.compile(r'(?P<end>kohtumees)A'),\\\n",
    "                                         re.compile(r'(?P<end>Pechmann)x'),\\\n",
    "                                         re.compile(r'(?P<end>pölli)Anni'),\\\n",
    "                                         re.compile(r'(?P<end>külla)Rauba'),\\\n",
    "                                         re.compile(r'(?P<end>kohtowannem)Jaak'),\\\n",
    "                                         re.compile(r'(?P<end>rannast)Leno'),\\\n",
    "                                         re.compile(r'(?P<end>wallast)Kiiwita'),\\\n",
    "                                         re.compile(r'(?P<end>wallas)Kristjan'),\\\n",
    "                                         re.compile(r'(?P<end>Pedoson)rahul'),\\\n",
    "                                         re.compile(r'(?P<end>pere)Jaan'),\\\n",
    "                                         re.compile(r'(?P<end>kohtu)poolest'),\\\n",
    "                                         re.compile(r'(?P<end>Kurrista)kaudo'),\\\n",
    "                                         re.compile(r'(?P<end>mölder)Gottlieb'),\\\n",
    "                                         re.compile(r'(?P<end>wöörmündri)Jaan'),\\\n",
    "                                         re.compile(r'(?P<end>Oinas)ja'),\\\n",
    "                                         re.compile(r'(?P<end>ette)Leenu'),\\\n",
    "                                         re.compile(r'(?P<end>Tommingas)peab'),\\\n",
    "                                         re.compile(r'(?P<end>wäljaja)Kotlep'),\\\n",
    "                                         re.compile(r'(?P<end>pea)A'),\\\n",
    "                                         re.compile(r'(?P<end>talumees)Nikolai')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in files from the distributed corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "\n",
    "with open('divided_corpus.txt', 'r', encoding = 'UTF-8') as f:\n",
    "    txt = f.readlines()\n",
    "\n",
    "for fileName in txt:\n",
    "    file, subdistribution = fileName.split(\":\")\n",
    "    files[file] = subdistribution.rstrip(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making `ner` and `wordner` layers from goldstandard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1\n",
      "File 2\n",
      "File 3\n",
      "File 4\n",
      "File 5\n",
      "File 6\n",
      "File 7\n",
      "File 8\n",
      "File 9\n",
      "File 10\n",
      "File 11\n",
      "File 12\n",
      "File 13\n",
      "File 14\n",
      "File 15\n",
      "File 16\n",
      "File 17\n",
      "File 18\n",
      "File 19\n",
      "File 20\n",
      "File 21\n",
      "File 22\n",
      "File 23\n",
      "File 24\n",
      "File 25\n",
      "File 26\n",
      "File 27\n",
      "File 28\n",
      "File 29\n",
      "File 30\n",
      "File 31\n",
      "File 32\n",
      "File 33\n",
      "File 34\n",
      "File 35\n",
      "File 36\n",
      "File 37\n",
      "File 38\n",
      "File 39\n",
      "File 40\n",
      "File 41\n",
      "File 42\n",
      "File 43\n",
      "File 44\n",
      "File 45\n",
      "File 46\n",
      "File 47\n",
      "File 48\n",
      "File 49\n",
      "File 50\n",
      "File 51\n",
      "File 52\n",
      "File 53\n",
      "File 54\n",
      "File 55\n",
      "File 56\n",
      "File 57\n",
      "File 58\n",
      "File 59\n",
      "File 60\n",
      "File 61\n",
      "File 62\n",
      "File 63\n",
      "File 64\n",
      "File 65\n",
      "File 66\n",
      "File 67\n",
      "File 68\n",
      "File 69\n",
      "File 70\n",
      "File 71\n",
      "File 72\n",
      "File 73\n",
      "File 74\n",
      "File 75\n",
      "File 76\n",
      "File 77\n",
      "File 78\n",
      "File 79\n",
      "File 80\n",
      "File 81\n",
      "File 82\n",
      "File 83\n",
      "File 84\n",
      "File 85\n",
      "File 86\n",
      "File 87\n",
      "File 88\n",
      "File 89\n",
      "File 90\n",
      "File 91\n",
      "File 92\n",
      "File 93\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "(\"'NerMorphFeatureTagger' object has no attribute 'retag'\", \"in the 'WordLevelNerTagger'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/estner/fex.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                     \u001b[0mfex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NerMorphFeatureTagger' object has no attribute 'process'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/estner/fex.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                         \u001b[0mfex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m                     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/tagger.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, text, status)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/tagger.py\u001b[0m in \u001b[0;36mmake_layer\u001b[0;34m(self, text, layers, status)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/estner/fex.py\u001b[0m in \u001b[0;36m_make_layer\u001b[0;34m(self, text, layers, status)\u001b[0m\n\u001b[1;32m    222\u001b[0m                                  \u001b[0mpost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_word_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                                  \u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_ending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mending\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                                  \u001b[0mpun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartofspeech\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"_Z_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshaped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/layer/span.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkey_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/layer/span.py\u001b[0m in \u001b[0;36mresolve_attribute\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mattribute_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute_mapping_for_elementary_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_object\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattribute_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_span\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/layer/layer.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_span\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/tagger.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, text, status)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mstore\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0mcreation\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/tagger.py\u001b[0m in \u001b[0;36mmake_layer\u001b[0;34m(self, text, layers, status)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'in the {!r}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/estner/word_level_ner_tagger.py\u001b[0m in \u001b[0;36m_make_layer\u001b[0;34m(self, text, layers, status)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0msnt_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mflattened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msnt_labels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/estner/fex.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    578\u001b[0m                         \u001b[0mfex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m                         \u001b[0mfex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0;31m# apply the feature templates.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: (\"'NerMorphFeatureTagger' object has no attribute 'retag'\", \"in the 'WordLevelNerTagger'\")"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def find(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)\n",
    "        \n",
    "counter = 0\n",
    "removed_layers = ['sentences', 'morph_analysis', 'wordner', 'compound_tokens', 'ner', 'words', 'tokens']\n",
    "for file in {key: value for key, value in files.items() if value == '1'}:\n",
    "    with open(find(file.replace(\".json\", \".txt\"), \"./vallakohtufailid/\"), 'r', encoding='UTF-8') as f:\n",
    "        text = Text(f.read())\n",
    "        text = text.tag_layer(['tokens'])\n",
    "        token_splitter.retag(text)\n",
    "        CompoundTokenTagger(tag_initials = False, tag_abbreviations = False, tag_hyphenations = False).tag(text)\n",
    "        text.tag_layer('morph_analysis')\n",
    "        nertagger.tag(text)\n",
    "        word_level_ner.tag(text)\n",
    "\n",
    "        text.add_layer(flatten(text['ner'], 'flat_ner'))\n",
    "        text.add_layer(flatten(text['wordner'], 'flat_wordner'))\n",
    "\n",
    "        for x in removed_layers:\n",
    "            text.pop_layer(x)\n",
    "        text_to_json(text, file=os.getcwd() + \"/vallakohtufailid_nertagger/\" + file)\n",
    "        counter += 1\n",
    "    print(f\"File {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1\n",
      "File 2\n",
      "File 3\n",
      "File 4\n",
      "File 5\n",
      "File 6\n",
      "File 7\n",
      "File 8\n",
      "File 9\n",
      "File 10\n",
      "File 11\n",
      "File 12\n",
      "File 13\n",
      "File 14\n",
      "File 15\n",
      "File 16\n",
      "File 17\n",
      "File 18\n",
      "File 19\n",
      "File 20\n",
      "File 21\n",
      "File 22\n",
      "File 23\n",
      "File 24\n",
      "File 25\n",
      "File 26\n",
      "File 27\n",
      "File 28\n",
      "File 29\n",
      "File 30\n",
      "File 31\n",
      "File 32\n",
      "File 33\n",
      "File 34\n",
      "File 35\n",
      "File 36\n",
      "File 37\n",
      "File 38\n",
      "File 39\n",
      "File 40\n",
      "File 41\n",
      "File 42\n",
      "File 43\n",
      "File 44\n",
      "File 45\n",
      "File 46\n",
      "File 47\n",
      "File 48\n",
      "File 49\n",
      "File 50\n",
      "File 51\n",
      "File 52\n",
      "File 53\n",
      "File 54\n",
      "File 55\n",
      "File 56\n",
      "File 57\n",
      "File 58\n",
      "File 59\n",
      "File 60\n",
      "File 61\n",
      "File 62\n",
      "File 63\n",
      "File 64\n",
      "File 65\n",
      "File 66\n",
      "File 67\n",
      "File 68\n",
      "File 69\n",
      "File 70\n",
      "File 71\n",
      "File 72\n",
      "File 73\n",
      "File 74\n",
      "File 75\n",
      "File 76\n",
      "File 77\n",
      "File 78\n",
      "File 79\n",
      "File 80\n",
      "File 81\n",
      "File 82\n",
      "File 83\n",
      "File 84\n",
      "File 85\n",
      "File 86\n",
      "File 87\n",
      "File 88\n",
      "File 89\n",
      "File 90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/tagger.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, text, status)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mstore\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0mcreation\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/tagger.py\u001b[0m in \u001b[0;36mmake_layer\u001b[0;34m(self, text, layers, status)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'in the {!r}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/estner/word_level_ner_tagger.py\u001b[0m in \u001b[0;36m_make_layer\u001b[0;34m(self, text, layers, status)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0msnt_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mflattened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msnt_labels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/estner/fex.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# apply the feature templates.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0mapply_templates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEMPLATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/estner/fex.py\u001b[0m in \u001b[0;36mapply_templates\u001b[0;34m(toks, templates)\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                             \u001b[0mtoken_lists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counter = 0\n",
    "removed_layers = ['sentences', 'morph_analysis', 'wordner', 'compound_tokens', 'ner', 'words', 'tokens']\n",
    "for file in files:\n",
    "    if int(files[file]) == 1:\n",
    "        with open(\"vallakohtufailid_json_flat/\" + file, 'r', encoding='UTF-8') as f:\n",
    "            text_import = json_to_text(f.read())\n",
    "            text = Text(text_import.text)\n",
    "            text = text.tag_layer(['tokens'])\n",
    "            token_splitter.retag(text)\n",
    "            CompoundTokenTagger(tag_initials = False, tag_abbreviations = False, tag_hyphenations = False).tag(text)\n",
    "            text.tag_layer('morph_analysis')\n",
    "            nertagger.tag(text)\n",
    "            word_level_ner.tag(text)\n",
    "            \n",
    "            text.add_layer(flatten(text['ner'], 'flat_ner'))\n",
    "            text.add_layer(flatten(text['wordner'], 'flat_wordner'))\n",
    "            \n",
    "            for x in removed_layers:\n",
    "                text.pop_layer(x)\n",
    "            text_to_json(text, file=os.getcwd() + \"/vallakohtufailid_nertagger/\" + file)\n",
    "            counter += 1\n",
    "    print(f\"File {counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the f1-scores\n",
    "Layer `ner` against `gold_ner` and `wordner` against `gold_wordner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [23403, 23554]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f55579ca384f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1927\u001b[0m     \"\"\"\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 256\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [23403, 23554]"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "y_test = []\n",
    "for file in os.listdir(\"./vallakohtufailid_nertagger/\"):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(\"./vallakohtufailid_nertagger/\" + str(file), 'r', encoding='UTF-8') as test, \\\n",
    "            open(\"./vallakohtufailid_json_flat/\" + str(file), 'r', encoding='UTF-8') as train:\n",
    "                test_import = json_to_text(test.read())\n",
    "                train_import = json_to_text(train.read())\n",
    "                \n",
    "                for i in range(len(train_import['flat_gold_wordner'])):\n",
    "                    y_train.append(train_import['flat_gold_wordner'][i].nertag[0])\n",
    "                for i in range(len(test_import['flat_wordner'])):\n",
    "                    y_test.append(test_import['flat_wordner'][i].nertag[0])\n",
    "\n",
    "labels = set(y_train + y_test)\n",
    "labels = sorted(labels,key=lambda name: (name[1:], name[0]))\n",
    "print(classification_report(y_train, y_test, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "179\n",
      "[]\n",
      "Text(text='Prottokoll sel 15mal Juunis 1881\\nKoos olid kohtu Peawanem: Jaan Saarmann\\nJaan Kreep\\nHans Simsohn\\nKirjutaja: H. Saarmu\\n\\n 32.\\n\\nKohtu laua ette astus Käru walla mees  Kaarel Liwersohn ja kaebas: et Hans Saarmann ja Hans Wanaisak on teda hirmsaste Kirna Reopalu kõrtsus peksnud ja et Toomas Wirkaus, Hans Poak ja Mihkel  Nass on seda peksmist pealt näinud.\\n Toomas Wirkaus, Hans Poak ja Mihkel Nass astusid ette ja ütlesid: et  Kaarel Liwersohni kaebdus tõsi on.\\nHans Saarmann ja Hans Wanaisak astusid ette ja ei wõinud seda mitte ära salata, seepärast, et need ülemal nimetud tunistuse mehed on seda pealt waatanud.\\nNemad lepisid selle moodiga ära, et Hans Saarmann ja Hans Wanaisaak maksid 11 Rubla 50 kop. kus sest rahast 10 Rubla Kaarel Liwersohnile sai tema peksu eest, ja 1 Rubla 50 kop käemestele.\\nJa on ka täidetud. \\n\\nKirna walla kogukonna kohtu nimel,\\nPeawanem: Jaan Saarmann XXX\\nJaan Kreeps XXX\\nHans Simsohn XXX\\nTunnistuseks kirjutaja: H. Saarmu /allkiri/\\n\\n')\n",
      "Text(text='Prottokoll sel 15mal Juunis 1881\\nKoos olid kohtu Peawanem: Jaan Saarmann\\nJaan Kreep\\nHans Simsohn\\nKirjutaja: H. Saarmu\\n\\n 32.\\n\\nKohtu laua ette astus Käru walla mees  Kaarel Liwersohn ja kaebas: et Hans Saarmann ja Hans Wanaisak on teda hirmsaste Kirna Reopalu kõrtsus peksnud ja et Toomas Wirkaus, Hans Poak ja Mihkel  Nass on seda peksmist pealt näinud.\\n Toomas Wirkaus, Hans Poak ja Mihkel Nass astusid ette ja ütlesid: et  Kaarel Liwersohni kaebdus tõsi on.\\nHans Saarmann ja Hans Wanaisak astusid ette ja ei wõinud seda mitte ära salata, seepärast, et need ülemal nimetud tunistuse mehed on seda pealt waatanud.\\nNemad lepisid selle moodiga ära, et Hans Saarmann ja Hans Wanaisaak maksid 11 Rubla 50 kop. kus sest rahast 10 Rubla Kaarel Liwersohnile sai tema peksu eest, ja 1 Rubla 50 kop käemestele.\\nJa on ka täidetud. \\n\\nKirna walla kogukonna kohtu nimel,\\nPeawanem: Jaan Saarmann XXX\\nJaan Kreeps XXX\\nHans Simsohn XXX\\nTunnistuseks kirjutaja: H. Saarmu /allkiri/\\n\\n')\n"
     ]
    }
   ],
   "source": [
    "print(len(train_import.flat_gold_wordner))\n",
    "print(len(test_import['flat_wordner']))\n",
    "print(list(set(train_import.flat_gold_wordner.text) - set(test_import.flat_wordner.text)))\n",
    "print(test_import)\n",
    "print(train_import)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

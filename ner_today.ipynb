{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import sklearn_crfsuite\n",
    "import re\n",
    "import nereval\n",
    "import pandas as pd\n",
    "\n",
    "from estnltk import Text\n",
    "from estnltk.taggers import NerTagger\n",
    "from estnltk.taggers import WordLevelNerTagger\n",
    "from estnltk.converters import text_to_json\n",
    "from estnltk.converters import json_to_text\n",
    "from estnltk.layer_operations import flatten\n",
    "from sklearn.metrics import classification_report\n",
    "from estnltk.taggers import Retagger\n",
    "from estnltk.taggers import CompoundTokenTagger\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nervaluate import Evaluator\n",
    "\n",
    "nertagger = NerTagger()\n",
    "word_level_ner = WordLevelNerTagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding TokenSplitter to make an equal amount of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenSplitter( Retagger ):\n",
    "    \"\"\"Splits tokens into smaller tokens based on regular expression patterns.\"\"\" \n",
    "    conf_param = ['patterns', 'break_group_name']\n",
    "    \n",
    "    def __init__(self, patterns, break_group_name:str='end'):\n",
    "        # Set input/output layers\n",
    "        self.input_layers = ['tokens']\n",
    "        self.output_layer = 'tokens'\n",
    "        self.output_attributes = ()\n",
    "        # Set other configuration parameters\n",
    "        if not (isinstance(break_group_name, str) and len(break_group_name) > 0):\n",
    "            raise TypeError('(!) break_group_name should be a non-empty string.')\n",
    "        self.break_group_name = break_group_name\n",
    "        # Assert that all patterns are regular expressions in the valid format\n",
    "        if not isinstance(patterns, list):\n",
    "            raise TypeError('(!) patterns should be a list of compiled regular expressions.')\n",
    "        # TODO: we use an adhoc way to verify that patterns are regular expressions \n",
    "        #       because there seems to be no common way of doing it both in py35 \n",
    "        #       and py36\n",
    "        for pat in patterns:\n",
    "            # Check for the existence of methods/attributes\n",
    "            has_match   = callable(getattr(pat, \"match\", None))\n",
    "            has_search  = callable(getattr(pat, \"search\", None))\n",
    "            has_pattern = getattr(pat, \"pattern\", None) is not None\n",
    "            for (k,v) in (('method match()',has_match),\\\n",
    "                          ('method search()',has_search),\\\n",
    "                          ('attribute pattern',has_pattern)):\n",
    "                if v is False:\n",
    "                    raise TypeError('(!) Unexpected regex pattern: {!r} is missing {}.'.format(pat, k))\n",
    "            symbolic_groups = pat.groupindex\n",
    "            if self.break_group_name not in symbolic_groups.keys():\n",
    "                raise TypeError('(!) Pattern {!r} is missing symbolic group named {!r}.'.format(pat, self.break_group_name))\n",
    "        self.patterns = patterns\n",
    "\n",
    "    def _change_layer(self, text, layers, status):\n",
    "        # Get changeble layer\n",
    "        changeble_layer = layers[self.output_layer]\n",
    "        # Iterate over tokens\n",
    "        add_spans    = []\n",
    "        remove_spans = []\n",
    "        for span in changeble_layer:\n",
    "            token_str = text.text[span.start:span.end]\n",
    "            for pat in self.patterns:\n",
    "                m = pat.search(token_str)\n",
    "                if m:\n",
    "                    break_group_end = m.end( self.break_group_name )\n",
    "                    if break_group_end > -1 and \\\n",
    "                       break_group_end > 0  and \\\n",
    "                       break_group_end < len(token_str):\n",
    "                        # Make the split\n",
    "                        add_spans.append( (span.start, span.start+break_group_end) )\n",
    "                        add_spans.append( (span.start+break_group_end, span.end) )\n",
    "                        remove_spans.append( span )\n",
    "                        # Once a token has been split, then break and move on to \n",
    "                        # the next token ...\n",
    "                        break\n",
    "        if add_spans:\n",
    "            assert len(remove_spans) > 0\n",
    "            for old_span in remove_spans:\n",
    "                changeble_layer.remove_span( old_span )\n",
    "            for new_span in add_spans:\n",
    "                changeble_layer.add_annotation( new_span )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter = TokenSplitter(patterns=[re.compile(r'(?P<end>[A-ZÕÄÖÜ]{1}\\w+)[A-ZÕÄÖÜ]{1}\\w+'),\\\n",
    "                                         re.compile(r'(?P<end>Piebenomme)metsawaht'),\\\n",
    "                                         re.compile(r'(?P<end>maa)peal'),\\\n",
    "                                         re.compile(r'(?P<end>reppi)käest'),\\\n",
    "                                         re.compile(r'(?P<end>Kiidjerwelt)J'),\\\n",
    "                                         re.compile(r'(?P<end>Ameljanow)Persitski'),\\\n",
    "                                         re.compile(r'(?P<end>mõistmas)Mihkel'),\\\n",
    "                                         re.compile(r'(?P<end>tema)Käkk'),\\\n",
    "                                         re.compile(r'(?P<end>Ahjawalla)liikmed'),\\\n",
    "                                         re.compile(r'(?P<end>kohtumees)A'),\\\n",
    "                                         re.compile(r'(?P<end>Pechmann)x'),\\\n",
    "                                         re.compile(r'(?P<end>pölli)Anni'),\\\n",
    "                                         re.compile(r'(?P<end>külla)Rauba'),\\\n",
    "                                         re.compile(r'(?P<end>kohtowannem)Jaak'),\\\n",
    "                                         re.compile(r'(?P<end>rannast)Leno'),\\\n",
    "                                         re.compile(r'(?P<end>wallast)Kiiwita'),\\\n",
    "                                         re.compile(r'(?P<end>wallas)Kristjan'),\\\n",
    "                                         re.compile(r'(?P<end>Pedoson)rahul'),\\\n",
    "                                         re.compile(r'(?P<end>pere)Jaan'),\\\n",
    "                                         re.compile(r'(?P<end>kohtu)poolest'),\\\n",
    "                                         re.compile(r'(?P<end>Kurrista)kaudo'),\\\n",
    "                                         re.compile(r'(?P<end>mölder)Gottlieb'),\\\n",
    "                                         re.compile(r'(?P<end>wöörmündri)Jaan'),\\\n",
    "                                         re.compile(r'(?P<end>Oinas)ja'),\\\n",
    "                                         re.compile(r'(?P<end>ette)Leenu'),\\\n",
    "                                         re.compile(r'(?P<end>Tommingas)peab'),\\\n",
    "                                         re.compile(r'(?P<end>wäljaja)Kotlep'),\\\n",
    "                                         re.compile(r'(?P<end>pea)A'),\\\n",
    "                                         re.compile(r'(?P<end>talumees)Nikolai')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in files from the distributed corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "\n",
    "with open('divided_corpus.txt', 'r', encoding = 'UTF-8') as f:\n",
    "    txt = f.readlines()\n",
    "\n",
    "for fileName in txt:\n",
    "    file, subdistribution = fileName.split(\":\")\n",
    "    files[file] = subdistribution.rstrip(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making `ner` and `wordner` layers from goldstandard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "removed_layers = ['sentences', 'morph_analysis', 'wordner', 'compound_tokens', 'ner', 'words', 'tokens']\n",
    "for file in {key: value for key, value in files.items() if value in ('1', '2', '3')}:\n",
    "    with open(find(file.replace(\".json\", \".txt\"), \"./vallakohtufailid/\"), 'r', encoding='UTF-8') as f:\n",
    "        text = Text(f.read())\n",
    "        if f == \"Tartu_V6nnu_Ahja_id3502_1882a.txt\":\n",
    "            text = text.replace('..', '. .')\n",
    "        \n",
    "        text = text.tag_layer(['tokens'])\n",
    "        token_splitter.retag(text)\n",
    "        CompoundTokenTagger(tag_initials = False, tag_abbreviations = False, tag_hyphenations = False).tag(text)\n",
    "        text.tag_layer('morph_analysis')\n",
    "        \n",
    "        nertagger.tag(text)\n",
    "        word_level_ner.tag(text)\n",
    "\n",
    "        text.add_layer(flatten(text['ner'], 'flat_ner'))\n",
    "        text.add_layer(flatten(text['wordner'], 'flat_wordner'))\n",
    "\n",
    "        for x in removed_layers:\n",
    "            text.pop_layer(x)\n",
    "        text_to_json(text, file=os.getcwd() + \"/vallakohtufailid_nertagger/\" + file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the f1-scores\n",
    "Layer `ner` against `gold_ner` and `wordner` against `gold_wordner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_not_working = ['J2rva_Tyri_V22tsa_id22177_1911a.json', \\\n",
    "                     'J2rva_Tyri_V22tsa_id18538_1894a.json', \\\n",
    "                     'J2rva_Tyri_V22tsa_id22155_1911a.json', \\\n",
    "                     'Saare_Kihelkonna_Kotlandi_id18845_1865a.json', \\\n",
    "                     'P2rnu_Halliste_Abja_id257_1844a.json', \\\n",
    "                     'Saare_Kaarma_Loona_id7575_1899a.json']\n",
    "#'Tartu_R6ngu_Aakre_id14648_1829a.json'\n",
    "#'Tartu_V6nnu_Ahja_id3502_1882a.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gold = []\n",
    "test = []\n",
    "gold_ner = []\n",
    "test_ner = []\n",
    "for file in sorted(os.listdir(\"./vallakohtufailid_nertagger/\")):\n",
    "    appendable_gold_ner = []\n",
    "    appendable_test_ner = []\n",
    "    if file.endswith(\".json\"):\n",
    "        if file in files_not_working:\n",
    "            continue\n",
    "        else:\n",
    "            with open(\"./vallakohtufailid_nertagger/\" + str(file), 'r', encoding='UTF-8') as f_test, \\\n",
    "                open(\"./vallakohtufailid_json_flat/\" + str(file), 'r', encoding='UTF-8') as f_gold:\n",
    "                    test_import = json_to_text(f_test.read())\n",
    "                    gold_import = json_to_text(f_gold.read())\n",
    "\n",
    "                    for i in range(len(gold_import['flat_gold_wordner'])):\n",
    "                        gold.append(gold_import['flat_gold_wordner'][i].nertag[0])\n",
    "                    for i in range(len(test_import['flat_wordner'])):\n",
    "                        test.append(test_import['flat_wordner'][i].nertag[0])\n",
    "                    \n",
    "                    for i in range(len(gold_import['flat_gold_ner'])):\n",
    "                        ner = gold_import['flat_gold_ner'][i]\n",
    "                        appendable_gold_ner.append({\"label\": ner.nertag[0], \"start\": int(ner.start), \"end\": int(ner.end)})\n",
    "                    for i in range(len(test_import['flat_ner'])):\n",
    "                        ner = test_import['flat_ner'][i]\n",
    "                        appendable_test_ner.append({\"label\": ner.nertag[0], \"start\": int(ner.start), \"end\": int(ner.end)})\n",
    "\n",
    "    gold_ner.append(appendable_gold_ner)\n",
    "    test_ner.append(appendable_test_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.94      0.96      0.95    133945\n",
      "       B-LOC       0.13      0.21      0.16       511\n",
      "       I-LOC       0.04      0.01      0.02       212\n",
      "      B-MISC       0.00      0.00      0.00       134\n",
      "      I-MISC       0.00      0.00      0.00       397\n",
      "       B-ORG       0.09      0.13      0.11      1391\n",
      "       I-ORG       0.06      0.10      0.07      1170\n",
      "       B-PER       0.61      0.57      0.59     11132\n",
      "       I-PER       0.64      0.51      0.57     10428\n",
      "\n",
      "    accuracy                           0.88    159320\n",
      "   macro avg       0.28      0.28      0.27    159320\n",
      "weighted avg       0.88      0.88      0.88    159320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = set(gold) \n",
    "sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))\n",
    "print(classification_report(gold, test, labels=sorted_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(gold_ner, test_ner, tags=['ORG', 'PER', 'MISC', 'LOC'])\n",
    "results, results_per_tag = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ent_type</th>\n",
       "      <th>partial</th>\n",
       "      <th>strict</th>\n",
       "      <th>exact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <td>9011.000000</td>\n",
       "      <td>8141.000000</td>\n",
       "      <td>7480.000000</td>\n",
       "      <td>8141.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incorrect</th>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3431.000000</td>\n",
       "      <td>2770.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2770.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missed</th>\n",
       "      <td>2257.000000</td>\n",
       "      <td>2257.000000</td>\n",
       "      <td>2257.000000</td>\n",
       "      <td>2257.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spurious</th>\n",
       "      <td>2052.000000</td>\n",
       "      <td>2052.000000</td>\n",
       "      <td>2052.000000</td>\n",
       "      <td>2052.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>possible</th>\n",
       "      <td>13168.000000</td>\n",
       "      <td>13168.000000</td>\n",
       "      <td>13168.000000</td>\n",
       "      <td>13168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <td>12963.000000</td>\n",
       "      <td>12963.000000</td>\n",
       "      <td>12963.000000</td>\n",
       "      <td>12963.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.695132</td>\n",
       "      <td>0.734861</td>\n",
       "      <td>0.577027</td>\n",
       "      <td>0.628018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.684310</td>\n",
       "      <td>0.723420</td>\n",
       "      <td>0.568044</td>\n",
       "      <td>0.618241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.689679</td>\n",
       "      <td>0.729096</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.623091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ent_type       partial        strict         exact\n",
       "correct     9011.000000   8141.000000   7480.000000   8141.000000\n",
       "incorrect   1900.000000      0.000000   3431.000000   2770.000000\n",
       "partial        0.000000   2770.000000      0.000000      0.000000\n",
       "missed      2257.000000   2257.000000   2257.000000   2257.000000\n",
       "spurious    2052.000000   2052.000000   2052.000000   2052.000000\n",
       "possible   13168.000000  13168.000000  13168.000000  13168.000000\n",
       "actual     12963.000000  12963.000000  12963.000000  12963.000000\n",
       "precision      0.695132      0.734861      0.577027      0.628018\n",
       "recall         0.684310      0.723420      0.568044      0.618241\n",
       "f1             0.689679      0.729096      0.572500      0.623091"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

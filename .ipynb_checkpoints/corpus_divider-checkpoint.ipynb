{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lisanüanss: lõplik jaotus peaks olema selline, et seal leidub üks alamosa, kus ei ole ühtegi käsitsi morf ühestatud protokolli. Käsitsi morf ühestatud protokollide nimekiri on siin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_protocols_tagged_by_hand(file = \"k2sitsi_morfiga_protokollid.txt\"):\n",
    "    protocols_tagged_by_hand = []\n",
    "    with open(file, 'r', encoding=\"UTF-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        if line != \"\" and (line != \"k2sitsi_morfiga_protokollid2:\" and line != \"k2sitsi_morfiga_protokollid1:\"):\n",
    "            line += \".json\"\n",
    "            protocols_tagged_by_hand.append(line)\n",
    "            \n",
    "    return protocols_tagged_by_hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma arvan, et \"ideaalseks jaotuseks\" võibki esialgu võtta nimeüksuste jaotuse liikidesse kogu korpusel, mille annab skript \"check_ner_layers.ipynb\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_whole_corpus_distribution(filenames = os.listdir(\"vallakohtufailid_json/\")):\n",
    "    all_annotations = []\n",
    "    for file in filenames:\n",
    "        with open(\"vallakohtufailid_json/\" + file, 'r', encoding=\"UTF-8)\") as f:\n",
    "            data = json.load(f)\n",
    "            for dictionary in data.get('layers')[0].get('spans'):\n",
    "                all_annotations.append(dictionary.get('annotations')[0].get('nertag'))\n",
    "    per = all_annotations.count(\"PER\") / len(all_annotations) * 100\n",
    "    loc_org = all_annotations.count(\"LOC_ORG\") / len(all_annotations) * 100\n",
    "    loc = all_annotations.count(\"LOC\") / len(all_annotations) * 100\n",
    "    org = all_annotations.count(\"ORG\") / len(all_annotations) * 100\n",
    "    misc = all_annotations.count(\"MISC\") / len(all_annotations) * 100\n",
    "\n",
    "    ideal_distribution = {'PER': per, 'LOC_ORG' : loc_org, 'LOC' : loc, 'ORG' : org, 'MISC' : misc}\n",
    "    \n",
    "    return ideal_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kõigepealt tekita sõnastik, kus on iga json faili kohta nimeüksuste statistika liikide kaupa \"gold_ner\" kihist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_document_ne_statistics(filenames = os.listdir(\"vallakohtufailid_json/\")):\n",
    "    statistics = dict()\n",
    "    for file in filenames:\n",
    "        ner_annotations = []\n",
    "        with open(\"vallakohtufailid_json/\" + file, 'r', encoding=\"UTF-8)\") as f:\n",
    "            data = json.load(f)\n",
    "            for dictionary in data.get('layers')[0].get('spans'):\n",
    "                ner_annotations.append(dictionary.get('annotations')[0].get('nertag'))\n",
    "\n",
    "        statistics_for_file = dict()\n",
    "        for annotation in set(ner_annotations):\n",
    "            statistics_for_file[str(annotation)] = ner_annotations.count(annotation)\n",
    "        statistics[file] = statistics_for_file\n",
    "\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics, the ideal distribution and the protocols tagged by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statistics = find_document_ne_statistics()\n",
    "ideal_distribution = calculate_whole_corpus_distribution()\n",
    "protocols_tagged_by_hand = read_protocols_tagged_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_items_in_lists(A, B, i, j):\n",
    "    temp_b = B[j]\n",
    "    B[j] = A[i]\n",
    "    A[i] = temp_b\n",
    "    return A, B\n",
    "\n",
    "def improve_scores(largest, second_largest):\n",
    "    score_largest_old = calculate_score(largest)\n",
    "    score_second_largest_old = calculate_score(second_largest)\n",
    "    for i in range(len(largest)):\n",
    "        for j in range(len(second_largest)):\n",
    "            largest, second_largest = swap_items_in_lists(largest, second_largest, i, j)\n",
    "            \n",
    "            score_largest_new = calculate_score(largest)\n",
    "            score_second_largest_new = calculate_score(second_largest)\n",
    "\n",
    "            if score_largest_old > score_largest_new and score_second_largest_old > score_second_largest_new:\n",
    "                print(f\"{i, j} Skoor paranes mõlema alamkorpuse lõikes.\")\n",
    "                score_largest_old = score_largest_new\n",
    "                score_second_largest_old = score_second_largest_new\n",
    "                i += 1\n",
    "            elif score_largest_old < score_largest_new and score_second_largest_old > score_second_largest_new:\n",
    "                print(f\"{i, j} Skoor paranes ainult esimeses massiivis.\")\n",
    "                swap_items_in_lists(largest, second_largest, i, j)\n",
    "                score_largest_old = score_largest_old\n",
    "                score_second_largest_old = score_second_largest_old\n",
    "            elif score_largest_old > score_largest_new and score_second_largest_old < score_second_largest_new:\n",
    "                print(f\"{i, j} Skoor paranes ainult teises massiivis.\")\n",
    "                swap_items_in_lists(largest, second_largest, i, j)\n",
    "                score_largest_old = score_largest_old\n",
    "                score_second_largest_old = score_second_largest_old\n",
    "            else:\n",
    "                print(f\"{i, j} Skoor ei paranenud kummaski massiivis.\")\n",
    "                swap_items_in_lists(largest, second_largest, i, j)\n",
    "                score_largest_old = score_largest_old\n",
    "                score_second_largest_old = score_second_largest_old\n",
    "    return largest, second_largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_of_hand_tagged(least_hand_tagged, other_files):\n",
    "    for i, x in enumerate(set(least_hand_tagged).intersection(protocols_tagged_by_hand)):\n",
    "        print(i+1, x)\n",
    "        score_old = calculate_score(least_hand_tagged)\n",
    "\n",
    "        for files in other_files:\n",
    "            breaking = \"no\"\n",
    "            score_old_output = calculate_score(files)\n",
    "\n",
    "            for j, y in enumerate(files):\n",
    "                if y in protocols_tagged_by_hand:\n",
    "                    continue\n",
    "                else:\n",
    "                    swap_items_in_lists(least_hand_tagged, files, i, j)\n",
    "                    score_new = calculate_score(least_hand_tagged)\n",
    "                    score_new_output = calculate_score(files)\n",
    "                    if score_new < score_old and score_new_output < score_old_output:\n",
    "                        print(\"Leidsin vaste! Vahetasin failiga\", y)\n",
    "                        breaking = \"yes\"\n",
    "                        break\n",
    "                    elif score_new == score_old and score_new_output == score_old_output or \\\n",
    "                         abs(score_new - score_old) < 0.05 and abs(score_new_output-score_old_output) < 0.05:\n",
    "                        print(\"Leidsin vaste! Vahetasin failiga\", y)\n",
    "                        breaking = \"yes\"\n",
    "                        break\n",
    "                    else:\n",
    "                        swap_items_in_lists(files, least_hand_tagged, i, j)\n",
    "                        continue\n",
    "            if breaking == \"yes\":\n",
    "                break\n",
    "    return least_hand_tagged, other_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seejärel loo funktsioon, mis leiab (eelmise sõnastiku abil) nimeüksuste proportsionaalse jaotuse etteantud alamkorpuses. Sisendiks on alamkorpus ehk järjend json failide nimedest ja tulemusena tagastatakse sõnastik, kus on iga nimeüksuse liigi kohta toodud selle osakaal alamkorpuses: { 'PER': x, 'LOC': y, ... }. Osakaal on siis jagatis: {nimeüksuseid antud liigist} / {kõik nimeüksused alamkorpuses} -- kas protsendina või arvuna vahemikust 0.0 kuni 1.0, kuidas järgnevatel etappidel mugavam tundub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_proportions(filenames):\n",
    "    statistics_for_proportion = {}\n",
    "    \n",
    "    for file in filenames:\n",
    "        statistics_for_proportion[file] = statistics[file]\n",
    "\n",
    "    all_annotations = list()\n",
    "    for item in statistics_for_proportion.values():\n",
    "        for key in item:\n",
    "            for i in range(item[key] + 1):\n",
    "                all_annotations.append(key)\n",
    "                \n",
    "    proportions = dict()\n",
    "    for file in statistics_for_proportion:\n",
    "        for key in statistics_for_proportion[file].keys():\n",
    "            proportion = all_annotations.count(key) / len(all_annotations) * 100\n",
    "            proportions[key] = proportion\n",
    "        \n",
    "    return proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neljas funktsioon: juhujaotuse genereerimine. Jagab sisendiks antud korpuse dokumendid (järjend json failide nimedest) N juhuslikuks alamosaks -- nii, et iga alamosa koosneb sellistest dokumentidest, mida mujal alamosades ei ole ning kogu korpuse dokumendid on kaetud. Selleks: a) järjesta kõik dokumendid juhuslikult random.sample funktsiooni abil (et eksperiment oleks korratav, fikseeri enne iga juhuslikku järjestamist random.seed mingi kindla väärtusega). b) jaga järjestus N alamosaks nii, et igas alamosas on enam-vähem võrdne arv dokumente. Selleks on mitmeid viise (vt nt siit), aga oluline on, et viimane alamosa ei tuleks liiga väike. Tagasta tulemus (N liikmeline list, mille iga liige on omakorda dokumentide list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_even_chunks(filenames, n):\n",
    "    files = []\n",
    "    last = 0\n",
    "    for i in range(1, n+1):\n",
    "        current = int(round(i* (len(filenames) / n)))\n",
    "        files.append(filenames[last:current])\n",
    "        last = current\n",
    "    return files\n",
    "\n",
    "def generate_random_division(filenames, n):\n",
    "    random_distributions = []\n",
    "    for i in range(n):\n",
    "        filenames = random.sample(filenames, len(filenames))\n",
    "        random_distributions.append(filenames)\n",
    "    \n",
    "    return random_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolmas funktsioon: saab sisendiks alamkorpuse (järjendi json failide nimedest) ja arvutab sellele skoori, mis näitab seda, kui lähedal on nimeüksuste proportsionaalne jaotus nö ideaalsele jaotusele. Algoritm võib lihtsalt olla selline, et leiab iga nimeüksuste tüübi kohta, mitu protsendipunkti erineb osakaal antud alamkorpuses nimeüksuse liigi osakaalust selles nö ideaalses jaotuses, ja summeerib erinevused. Skoor töötab siis nii, et mida väiksem erinevuste summa, seda parem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(filenames):\n",
    "    proportions = calculate_proportions(filenames)\n",
    "    score = 0\n",
    "    for proportion in proportions:\n",
    "        ideal_distribution_proportion = ideal_distribution[proportion]\n",
    "        current_proportion = proportions[proportion]\n",
    "        \n",
    "        if current_proportion == ideal_distribution_proportion:\n",
    "            score += 0\n",
    "        else:\n",
    "            score += abs(ideal_distribution_proportion - current_proportion)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 0\n",
      "7.49810704503649\n",
      "9.918086072691803\n",
      "7.5555920651177555\n",
      "6.59971016626813\n",
      "11.428219874925578\n",
      "8.667367929638713\n",
      "Score of sub-corpus: 8.61118052561308\n",
      " \n",
      "10.603654082464963\n",
      "7.5346799108846385\n",
      "6.602481512183512\n",
      "10.360863129648\n",
      "9.178850105842189\n",
      "7.332724881744491\n",
      "Score of sub-corpus: 8.602208937127966\n",
      " \n",
      "8.752548402461386\n",
      "7.403612076075569\n",
      "8.406346014189152\n",
      "8.58044886933627\n",
      "8.758484246007118\n",
      "9.385208091838054\n",
      "Score of sub-corpus: 8.547774616651258\n",
      " \n",
      "6.8019504097935535\n",
      "7.676950409793554\n",
      "11.904403512246654\n",
      "11.547089541240748\n",
      "6.6362453555586\n",
      "7.148161850344615\n",
      "Score of sub-corpus: 8.619133513162954\n",
      " \n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "print(\"Seed:\", seed)\n",
    "\n",
    "subdistributions = list()\n",
    "random_distributions = generate_random_division(sorted(os.listdir(\"vallakohtufailid_json/\")), 4)\n",
    "for distribution in random_distributions:\n",
    "    distribution = n_even_chunks(distribution, 6)\n",
    "    subdistributions.append(distribution)\n",
    "    \n",
    "sums_of_distributions = list()\n",
    "list_of_scores = list()\n",
    "\n",
    "for distribution in subdistributions:\n",
    "    scores = dict()\n",
    "    for files in distribution:\n",
    "        score = calculate_score(files)\n",
    "        scores[score] = files\n",
    "        print(score)\n",
    "    list_of_scores.append(scores)\n",
    "    print(f\"Score of sub-corpus: {sum(scores.keys()) / len(scores.keys())}\")\n",
    "    print(\" \")\n",
    "\n",
    "for listofscores in list_of_scores:    \n",
    "    sums_of_distributions.append(sum(listofscores.keys()) / len(listofscores.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kõigepealt genereeri mingi arv korpuse juhuslikke jaotuseid 6-ks alamosaks. Nt N=10 tükki.\n",
    "\n",
    "2. Leia iga jaotuse skoor. Jaotuse skoor = leia kõigi kuue alamosa skoorid skoorimise funktsiooni abil ning liida kokku. Kui peaks juhtuma, et mõnele jaotusele trehvabki kohe väga väike skoor (nt kogu-erinevus <= 5 protsendipunkti), siis tasub protsess seisma panna ja tulemustele peale vaadata, st äkki ongi sobiv jaotus käes. Aga kui nii ei lähe, siis edasi:\n",
    "\n",
    "3. Vali välja kõige kõrgema skoori saanud jaotus.\n",
    "\n",
    "4. Iteratiivselt: 4.1) Leia jaotusest kaks alamosa, mille erinevus nö ideaalsest jaotusest on kõige suurem, 4.2) paarita mõlema jaotuse kõiki dokumente omavahel (topelt-tsükkel) ja leia iga paari (x1, y1) puhul, milliseks muutub mõlema alamosa skoor, kui vahetada dokumendid alamosades. 4.3) Teosta ümberpaigutus, mis parandas skoori kõige rohkem. 4.4) Leia uuesti kogu jaotuse skoor; kui see on väga väike (nt kogu-erinevus <= 5 protsendipunkti), siis seiska protsess ja vaatame tulemustele peale. Vastasel juhul pöördu tagasi punkti 4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smallest_score = list_of_scores[sums_of_distributions.index(min(sums_of_distributions))]\n",
    "\n",
    "\n",
    "while (sum(smallest_score.keys()) / len(smallest_score.keys())) >= 5:\n",
    "    keys_list = sorted(smallest_score.keys())\n",
    "    largest_key = keys_list[-1]\n",
    "    second_largest_key = keys_list[-2]\n",
    "    \n",
    "    largest = smallest_score[largest_key]\n",
    "    second_largest = smallest_score[second_largest_key]\n",
    "    \n",
    "    print(f\"Algsed skoorid on {keys_list[-1]} ning {keys_list[-2]}.\")\n",
    "    largest, second_largest = improve_scores(largest, second_largest)\n",
    "    score_after_improving_largest = calculate_score(largest)\n",
    "    score_after_improving_second_largest = calculate_score(second_largest)\n",
    "    print(f\"Skoorid pärast parandamist on {score_after_improving_largest} ning {score_after_improving_second_largest}.\")\n",
    "    del smallest_score[largest_key]\n",
    "    del smallest_score[second_largest_key]\n",
    "    smallest_score[score_after_improving_largest] = largest\n",
    "    smallest_score[score_after_improving_second_largest] = second_largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 L22ne_Martna_Martna_id14205_1869a.json\n",
      "Leidsin vaste! Vahetasin failiga V6ru_P6lva_K2hri_id21539_1850a.json\n",
      "2 Viljandi_Pilistvere_K6o_id24950_1843a.json\n",
      "Leidsin vaste! Vahetasin failiga Tartu_V6nnu_Ahja_id16984_1884a.json\n",
      "3 Tartu_R6ngu_Aakre_id12559_1827a.json\n",
      "Leidsin vaste! Vahetasin failiga Tartu_V6nnu_Ahja_id17632_1885a.json\n",
      "4 Tartu_V6nnu_Kiidj2rve_id24727_1866a.json\n",
      "Leidsin vaste! Vahetasin failiga Harju_Kose_Kose-Uuem6isa_id10389_1870a.json\n",
      "5 Viru_Haljala_Vihula_id5807_1876a.json\n",
      "6 Harju_Kose_Kose-Uuem6isa_id5292_1869a.json\n",
      "Leidsin vaste! Vahetasin failiga Tartu_V6nnu_Ahja_id18911_1887a.json\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "smallest_score = list_of_scores[sums_of_distributions.index(min(sums_of_distributions))]\n",
    "\n",
    "final_files_distribution = []\n",
    "\n",
    "number_of_hand_tagged_protocols = {}\n",
    "for files in smallest_score.values():\n",
    "    number_of_hand_tagged_protocols[len(set(files).intersection(protocols_tagged_by_hand))] = files\n",
    "\n",
    "least_hand_tagged_old = number_of_hand_tagged_protocols[(min(number_of_hand_tagged_protocols.keys()))]\n",
    "\n",
    "least_hand_tagged, other_files = get_rid_of_hand_tagged(least_hand_tagged_old, number_of_hand_tagged_protocols.values())\n",
    "\n",
    "print(len(set(least_hand_tagged).intersection(protocols_tagged_by_hand))) #SIIN ERROR (?)\n",
    "\n",
    "final_files.append(least_hand_tagged)\n",
    "for files in other_files:\n",
    "    final_files.append(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

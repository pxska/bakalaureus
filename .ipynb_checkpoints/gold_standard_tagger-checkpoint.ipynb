{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:font_manager.py:1423: Generating new fontManager, this may take some time...\n",
      "INFO:font_manager.py:1080: Failed to extract font properties from /System/Library/Fonts/LastResort.otf: tuple indices must be integers or slices, not str\n",
      "INFO:font_manager.py:1080: Failed to extract font properties from /System/Library/Fonts/Apple Color Emoji.ttc: In FT2Font: Could not set the fontsize (error code 0x17)\n",
      "INFO:font_manager.py:1080: Failed to extract font properties from /System/Library/Fonts/Supplemental/NISC18030.ttf: In FT2Font: Could not set the fontsize (error code 0x17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kristjanparnmets/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from estnltk import EnvelopingBaseSpan\n",
    "from estnltk import Text, Layer, Annotation, EnvelopingSpan, Span\n",
    "from estnltk.converters import text_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. P2rnu_Tori_Tori_id22633_1866a.txt: 23 24\n",
      "2. Tartu_Kodavere_Pala_id23275_1872a.txt: 20 21\n",
      "3. Harju_Hageri_Kohila_id4010_1890a.txt: 76 77\n",
      "4. J2rva_Ambla_Ambla_id6188_1889a.txt: 24 27\n",
      "5. Tartu_Torma_Avinurme_id2545_1868a.txt: 25 26\n",
      "6. Tartu_Kodavere_Alatskivi_id2193_1878a.txt: 24 27\n",
      "7. Tartu_V6nnu_Ahja_id20417_1888a.txt: 9 10\n",
      "8. J2rva_Tyri_V22tsa_id22259_1913a.txt: 21 22\n",
      "9. Tartu_Kodavere_Alatskivi_id7763_1879a.txt: 9 10\n",
      "10. J2rva_Ambla_Uudekyla_id13485_1867a.txt: 39 40\n",
      "11. Viljandi_K6pu_Suure-K6pu_id7189_1884a.txt: 10 11\n",
      "12. Tartu_Kodavere_Alatskivi_id11023_1880a.txt: 24 25\n",
      "13. Viljandi_K6pu_Suure-K6pu_id7202_1884a.txt: 13 15\n",
      "14. Viljandi_Paistu_Holstre_id6601_1827a.txt: 10 11\n",
      "15. Harju_Juuru_Juuru_id19451_1886a.txt: 13 14\n",
      "16. L22ne_Kullamaa_Piirsalu_id7871_1890a.txt: 66 67\n",
      "17. J2rva_Ambla_Ambla_id5939_1888a.txt: 27 28\n",
      "18. Harju_J6el2htme_J6el2htme_id7364_1868a.txt: 8 9\n",
      "Kaust vallakohus_esimene on läbitud.\n",
      "19. P2rnu_Halliste_Pornuse_id4791_1869a.txt: 9 10\n",
      "20. J2rva_Tyri_S2revere_id7087_1884a.txt: 46 47\n",
      "21. Saare_Kaarma_Loona_id7599_1910a.txt: 17 18\n",
      "22. Saare_Kihelkonna_Loona_id5138_1875a.txt: 12 14\n",
      "23. Tartu_Laiuse_Kivij2rve_id5382_1863a.txt: 12 13\n",
      "Kaust vallakohus_teine on läbitud.\n",
      "24. Saare_K2rla_K2rla_id5732_1827a.txt: 13 14\n",
      "25. Harju_Hageri_Kohila_id4769_1886a.txt: 57 58\n",
      "26. Tartu_Kodavere_Alatskivi_id6197_1879a.txt: 14 15\n",
      "27. V6ru_R6uge_Saaluse_id9577_1878a.txt: 16 17\n",
      "28. Viljandi_P6ltsamaa_Adavere_id12307_1895a.txt: 9 10\n",
      "29. V6ru_P6lva_Kiuma_id7860_1880a.txt: 16 17\n",
      "30. Tartu_Otep22_Pyhaj2rve_id4865_1885a.txt: 39 41\n",
      "31. L22ne_Ridala_Sinalepa_id24367_1884a.txt: 33 34\n",
      "32. J2rva_Tyri_S2revere_id5632_1881a.txt: 26 27\n",
      "33. P2rnu_Audru_V6lla_id6632_1878a.txt: 17 20\n",
      "34. Tartu_R6ngu_Aakre_id4733_1889a.txt: 13 14\n",
      "35. Tartu_R6ngu_Aakre_id6568_1826a.txt: 6 7\n",
      "36. Tartu_Torma_Avinurme_id2512_1868a.txt: 23 24\n",
      "37. P2rnu_T6stamaa_Seli_id10936_1887a.txt: 14 15\n",
      "38. J2rva_Ambla_Ambla_id6086_1888a.txt: 30 31\n",
      "39. V6ru_Urvaste_Vaabina_id785_1876a.txt: 12 13\n",
      "40. Harju_Kose_Palvere_id23829_1887a.txt: 73 74\n",
      "41. J2rva_Ambla_Ambla_id7313_1886a.txt: 10 11\n",
      "42. Tartu_Kodavere_Alatskivi_id6169_1879a.txt: 10 11\n",
      "43. Viljandi_Paistu_Holstre_id11504_1848a.txt: 6 7\n",
      "44. Tartu_Torma_Avinurme_id25282_1843a.txt: 269 270\n",
      "45. V6ru_R2pina_R2pina_id1164_1862a.txt: 13 14\n",
      "46. Tartu_Kodavere_Pala_id21272_1869a.txt: 8 9\n",
      "47. J2rva_Tyri_S2revere_id6061_1882a.txt: 87 88\n",
      "48. Harju_J6el2htme_J6el2htme_id6475_1868a.txt: 31 34\n",
      "49. Viljandi_Paistu_Holstre_id1818_1869a.txt: 12 13\n",
      "50. Harju_Jyri_Rae_id277_1874a.txt: 15 16\n",
      "51. Viljandi_K6pu_Suure-K6pu_id7185_1884a.txt: 14 17\n",
      "52. V6ru_R2pina_R2pina_id5391_1912a.txt: 15 16\n",
      "53. Harju_Jyri_Rae_id6623_1890a.txt: 62 63\n",
      "54. Harju_Hageri_Kohila_id5581_1890a.txt: 28 30\n",
      "Kaust vallakohus_kolmas on läbitud.\n",
      "55. V6ru_Vastseliina_Misso_id5473_1885a.txt: 12 13\n",
      "56. Viljandi_P6ltsamaa_Pajusi_id2106_1870a.txt: 19 20\n",
      "57. Tartu_Laiuse_Kivij2rve_id13164_1866a.txt: 22 23\n",
      "58. Harju_J6el2htme_J6el2htme_id7375_1869a.txt: 21 22\n",
      "59. Viljandi_K6pu_Suure-K6pu_id4484_1883a.txt: 11 12\n",
      "60. Harju_Hageri_Kohila_id5466_1889a.txt: 30 32\n",
      "61. Tartu_V6nnu_Ahja_id18356_1886a.txt: 43 45\n",
      "62. Harju_J6el2htme_J6el2htme_id6437_1867a.txt: 12 13\n",
      "63. Saare_K2rla_K2rla_id7153_1827a.txt: 12 14\n",
      "64. Saare_Kihelkonna_Pidula_id5682_1890a.txt: 11 12\n",
      "65. Tartu_Kodavere_Ranna_id14144_1855a.txt: 34 35\n",
      "66. Tartu_R6ngu_Aakre_id5583_1889a.txt: 24 25\n",
      "67. V6ru_P6lva_Kiuma_id6097_1880a.txt: 30 31\n",
      "68. V6ru_R2pina_R2pina_id1170_1863a.txt: 15 16\n",
      "69. J2rva_Tyri_S2revere_id6495_1883a.txt: 28 29\n",
      "70. Tartu_N6o_Pangodi_id5252_1889a.txt: 9 10\n",
      "71. Saare_K2rla_K2rla_id5736_1827a.txt: 17 18\n",
      "72. Harju_Hageri_Kohila_id4966_1888a.txt: 15 16\n",
      "73. Tartu_Torma_Avinurme_id3955_1858a.txt: 15 16\n",
      "74. Tartu_Otep22_Pyhaj2rve_id1540_1884a.txt: 9 10\n",
      "75. L22ne_Kullamaa_Sooniste_id2083_1872a.txt: 7 8\n",
      "76. P2rnu_Halliste_Pornuse_id2902_1868a.txt: 11 12\n",
      "77. V6ru_P6lva_Kiuma_id7856_1880a.txt: 21 23\n",
      "78. Tartu_Kodavere_Alatskivi_id6838_1879a.txt: 21 22\n",
      "Kaust vallakohus_neljas on läbitud.\n",
      "Programm on lõpetanud oma töö.\n"
     ]
    }
   ],
   "source": [
    "# If the annotation contains newline character, then indexes will contain ';' at the linebreak (e.g. 388 393;394 398 )\n",
    "indexes_on_line_split = re.compile(r' (\\d+) (\\d+;\\d+ ){1,}(\\d+)$')\n",
    "\n",
    "def collect_annotations( in_f ):\n",
    "    annotations = []\n",
    "    split_lines_ahead = 0\n",
    "    for line in in_f:\n",
    "        line = line.rstrip('\\n')\n",
    "        items = line.split('\\t')\n",
    "        if split_lines_ahead > 0:\n",
    "            split_lines_ahead -= 1\n",
    "            last_item = annotations[-1]\n",
    "            new_tuple = (last_item[0],last_item[1],last_item[2],last_item[3]+line,last_item[4])\n",
    "            annotations[-1] = new_tuple\n",
    "            continue\n",
    "        if len(items) == 3:\n",
    "            indexes_str = items[1]\n",
    "            if indexes_str.count(';') > 0:\n",
    "                split_lines_ahead += indexes_str.count(';')\n",
    "            indexes_str = indexes_on_line_split.sub(' \\\\1 \\\\3', indexes_str)\n",
    "            tag, start, end = indexes_str.split()\n",
    "            annotations.append( (tag, start, end, items[2], items[0]) )\n",
    "    seen = set()\n",
    "    removed_duplicates_annotations = []\n",
    "    for a, b, c, d, e in annotations:\n",
    "        if not b in seen:\n",
    "            seen.add(b)\n",
    "            removed_duplicates_annotations.append((a, b, c, d, e))\n",
    "        else:\n",
    "            for index, item in enumerate(removed_duplicates_annotations):\n",
    "                if item[1] == b and item[2] > c:\n",
    "                    tuple_without_n = (a, b, c, d, e)\n",
    "                    item = tuple_without_n\n",
    "                    removed_duplicates_annotations[index] = item\n",
    "                elif item[1] == b and item[2] < c:\n",
    "                    tuple_without_n = (a, b, item[2], d, e)\n",
    "                    item = tuple_without_n\n",
    "                    removed_duplicates_annotations[index] = item\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "    annotations = sorted(list(set(removed_duplicates_annotations)), key=lambda x: int(x[1]))\n",
    "    return annotations\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "rownr = 1\n",
    "directories = [\"vallakohus_esimene\", \"vallakohus_teine\", \"vallakohus_kolmas\", \"vallakohus_neljas\"]\n",
    "for directory in directories:\n",
    "    path = cwd + \"/\" + directory + \"/\"\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(path + file, 'r', encoding=\"utf-8\") as txt, open(path + file.split(\".\")[0] + \".ann\", 'r', encoding=\"utf-8\") as ann:\n",
    "                textfile = txt.read().replace(u\"\\xa0\", u\" \")\n",
    "\n",
    "                # converting the text form .txt file into an EstNLTK Text object and giving it the \"words\" layer\n",
    "                text = Text(textfile)\n",
    "                text.meta['origin_directory'] = str(directory)\n",
    "                text = text.tag_layer(['words'])\n",
    "                \n",
    "                # creating NER layers\n",
    "                gold_ner_layer = Layer(name=\"gold_ner\", text_object=text, attributes=['nertag'])\n",
    "                gold_wordner_layer = Layer(name=\"gold_wordner\", text_object=text, attributes=['nertag'], parent=\"words\")\n",
    "                \n",
    "                #fixing annotations \n",
    "                fixed_annotations = collect_annotations(ann)\n",
    "\n",
    "                annotation_dictionary = {}\n",
    "                for annotation in fixed_annotations:\n",
    "                    trigger = annotation[4]\n",
    "                    location = annotation[0] + \" \" + annotation[1] + \" \" + annotation[2]\n",
    "                    entity = annotation[3]\n",
    "                    annotation_dictionary[trigger] = [location, entity]\n",
    "\n",
    "                for key in annotation_dictionary:\n",
    "                    name = []\n",
    "                    location, entity = annotation_dictionary.get(key)\n",
    "    \n",
    "                    ner, startIndex, endIndex = location.split(\" \")\n",
    "                    \n",
    "                    #TODO: Üks probleemidest on hetkel siin - kui entity sisaldab punkti, siis peab punktile ümber lisama tühikud v.a kui entity on eesnimi ja perekonnanimi kokkukirjutatud:\n",
    "                    #nt A.Allas või P.Peetrik, kuid peab tühiku lisama siis, kui punkt on perekonnast eraldi, nt A. Allas või P. Peetrik. Samuti peab tühiku lisama siis, kui punkte on rohkem\n",
    "                    #kui üks, näiteks T.S.R. peaks olema T . S . R . (aga lõpust ilma tühikuta!!!)\n",
    "                    \n",
    "                    #UPDATE 02.11: Praegu initsiaalidega (nt L. E.) ja T.S.R-iga töötab ilusti, aga nt failis J2rva_Ambla_Ambla_id6188_1889a.json ei lisa ta stringist \"Kuru koguk. kohus\"\n",
    "                    #\"kohus\" osa.\n",
    "                    if \".\" in entity:\n",
    "                        if entity.count(\".\") > 1:\n",
    "                            if \". \" in entity:\n",
    "                                entity = re.sub(r'(\\.)', r' \\\\1', entity).rstrip()\n",
    "                            else:\n",
    "                                entity = re.sub(r'(\\.)', r' \\\\1 ', entity).rstrip()\n",
    "                        \n",
    "                            '''\n",
    "                        if \" \" in entity: \n",
    "                            entity = re.sub(\"\\.\", \" \\. \", entity)\n",
    "                        else:\n",
    "                            if entity.count(\".\") > 1:\n",
    "                                entity = re.sub(\"\\.\", \" \\. \", entity).rstrip()\n",
    "                            else:\n",
    "                                continue'''\n",
    "                    \n",
    "                    for i in range(len(text.words)):\n",
    "                        if text.words[i].start == (int(startIndex) - text.text[:int(text.words[i].start)].count(\"\\n\")):\n",
    "                                                   \n",
    "                            preceding_newlines = text.text[:int(text.words[i].start)].count(\"\\n\")\n",
    "                            startIndex = int(startIndex) - int(preceding_newlines)\n",
    "                            endIndex = int(endIndex) - int(preceding_newlines)\n",
    "                            \n",
    "                            if text.words[i].start == startIndex:\n",
    "                                if text.words[i].end == endIndex:\n",
    "                                    base_span = EnvelopingBaseSpan([text.words[i].base_span])\n",
    "                                    name = [text.words[i]] #adding the name element(s) to a list to use in the wordner dictionary later on\n",
    "                                else:\n",
    "                                    if text.words[i+1].end == endIndex: #if the next word's endindex is the same as the endindex in the .ann file, add a new span\n",
    "                                        name = [text.words[i], text.words[i+1]]\n",
    "                                    else:\n",
    "                                        for j in range(len(entity.split(\" \"))):\n",
    "                                            name.append(text.words[i+j])\n",
    "                                    base_span = EnvelopingBaseSpan([s.base_span for s in name])\n",
    "                                new_span = EnvelopingSpan(base_span, layer=gold_ner_layer)\n",
    "\n",
    "                                dictionary_for_wordner = dict()\n",
    "                                if ner == \"Isik\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"PER\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-PER\" \n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-PER\"\n",
    "                                if ner == \"KO_koht\" or ner == \"KO_org\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"LOC_ORG\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-LOC_ORG\"\n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-LOC_ORG\"\n",
    "                                if ner == \"Koht\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"LOC\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-LOC\"\n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-LOC\"\n",
    "                                if ner == \"Org\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"ORG\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-ORG\" \n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-ORG\"\n",
    "                                if ner == \"Muu\" or ner == \"Teadmata\" or ner == \"ese\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"MISC\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-MISC\" \n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-MISC\"\n",
    "                                gold_ner_layer.add_span(new_span)\n",
    "                                break\n",
    "                text.add_layer(gold_ner_layer)\n",
    "                if len(text.gold_ner) != len(fixed_annotations):\n",
    "                    print(str(rownr) + \". \" + str(file) + \":\", len(text.gold_ner), len(fixed_annotations))\n",
    "                    rownr += 1\n",
    "                for i in range(0, len(text.words)):\n",
    "                    for key in dictionary_for_wordner.keys():\n",
    "                        new_span = Span(base_span=text.words[i].base_span, layer=gold_wordner_layer)\n",
    "\n",
    "                        if i == key:\n",
    "                            new_span.add_annotation(Annotation(new_span, nertag=str(dictionary_for_wordner.get(key))))\n",
    "                            gold_wordner_layer.add_span(new_span)\n",
    "                            break\n",
    "                        else:\n",
    "                            if i in dictionary_for_wordner.keys():\n",
    "                                continue\n",
    "                            else:\n",
    "                                new_span.add_annotation(Annotation(new_span, nertag=\"O\"))\n",
    "                            gold_wordner_layer.add_span(new_span)\n",
    "                            break\n",
    "                \n",
    "                text.add_layer(gold_wordner_layer)\n",
    "                text_to_json(text, file=cwd + \"/vallakohtufailid_json/\" + file.replace(\".txt\", \".json\"))\n",
    "    print(f\"Kaust {directory} on läbitud.\")\n",
    "print(\"Programm on lõpetanud oma töö.\")\n",
    "\n",
    "#TODO: Esineb probleem, kus ühe entity lõpuindeks on sama, mis teise entity algusindeks (nt ühel on 36, 45 ja teisel 45, 54). Sel juhul on need kaks stringi tekstis koos, nt AllasPeetrik -\n",
    "#need on vaja kuidagi eraldada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import sklearn_crfsuite\n",
    "import re\n",
    "import nereval\n",
    "import pandas as pd\n",
    "import CompoundTokenTaggerModule\n",
    "\n",
    "from estnltk import Text\n",
    "from estnltk.taggers import NerTagger\n",
    "from estnltk.taggers import WordLevelNerTagger\n",
    "from estnltk.converters import text_to_json\n",
    "from estnltk.converters import json_to_text\n",
    "from estnltk.layer_operations import flatten\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from nervaluate import Evaluator\n",
    "\n",
    "from estnltk.taggers.estner.ner_trainer import NerTrainer\n",
    "from estnltk.taggers.estner.model_storage_util import ModelStorageUtil\n",
    "from estnltk.core import DEFAULT_PY3_NER_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files don't work because the protocols are written in a different language, which the goldstandard didn't recognise, hence have no goldstandard tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_not_working = ['J2rva_Tyri_V22tsa_id22177_1911a.json', \\\n",
    "                     'J2rva_Tyri_V22tsa_id18538_1894a.json', \\\n",
    "                     'J2rva_Tyri_V22tsa_id22155_1911a.json', \\\n",
    "                     'Saare_Kihelkonna_Kotlandi_id18845_1865a.json', \\\n",
    "                     'P2rnu_Halliste_Abja_id257_1844a.json', \\\n",
    "                     'Saare_Kaarma_Loona_id7575_1899a.json', \\\n",
    "                     'J2rva_Tyri_V22tsa_id22266_1913a.json', \\\n",
    "                     'J2rva_Tyri_V22tsa_id22178_1912a.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "\n",
    "with open('divided_corpus.txt', 'r', encoding = 'UTF-8') as f:\n",
    "    txt = f.readlines()\n",
    "\n",
    "for fileName in txt:\n",
    "    file, subdistribution = fileName.split(\":\")\n",
    "    files[file] = subdistribution.rstrip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valmistan ette treenimistekste.\n",
      "Treenimistekstid defineeritud 133.85761189460754 sekundiga.\n",
      "\n",
      "\n",
      "Alustan nertaggeri treenimist.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected bytes, ImmutableList found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-68a89c5a2c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mnersettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNerTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnersettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtraining_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gold_wordner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"NerTagger treenitud {time.time() - start} sekundiga.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Setting up the new trained nertagger and defining layers to be removed later on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/estner/ner_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, texts, labels, layer, model_dir)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/estnltk/taggers/estner/crfsuiteutil.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, texts, correct_labels, mode_filename)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mnew_xseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_xseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpycrfsuite/_pycrfsuite.pyx\u001b[0m in \u001b[0;36mpycrfsuite._pycrfsuite.BaseTrainer.append\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/pycrfsuite/_pycrfsuite.cpython-37m-darwin.so\u001b[0m in \u001b[0;36mvector.from_py.__pyx_convert_vector_from_py_std_3a__3a_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py37/lib/python3.7/site-packages/pycrfsuite/_pycrfsuite.cpython-37m-darwin.so\u001b[0m in \u001b[0;36mstring.from_py.__pyx_convert_string_from_py_std__in_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected bytes, ImmutableList found"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "\n",
    "for subdistribution in [1, 2, 3, 4, 5]:\n",
    "    training_subdistributions = []\n",
    "    for y in [1, 2, 3, 4, 5]:\n",
    "        if y == subdistribution:\n",
    "            subdistribution_for_testing = y\n",
    "        else:\n",
    "            training_subdistributions.append(y)\n",
    "    \n",
    "    # Getting the filenames to be trained on from the files dictionary.\n",
    "    filenames = {key: value for key, value in files.items() if int(value) in training_subdistributions}\n",
    "    \n",
    "    # Creating training_texts from the aforementioned filenames.\n",
    "    print(\"Valmistan ette treenimistekste.\")\n",
    "    start = time.time()\n",
    "    training_texts = []\n",
    "    for filename in filenames:\n",
    "        with open('./vallakohtufailid_json_flat/' + str(filename), 'r', encoding='UTF-8') as file:\n",
    "            if filename in files_not_working:\n",
    "                continue\n",
    "            else:\n",
    "                training_texts.append(CompoundTokenTaggerModule.tag_text(json_to_text(file.read())))\n",
    "    print(f\"Treenimistekstid defineeritud {time.time() - start} sekundiga.\")\n",
    "    \n",
    "    # Setting up the trainer and training.\n",
    "    print(\"\\n\\nAlustan nertaggeri treenimist.\")\n",
    "    start = time.time()\n",
    "    model_dir=DEFAULT_PY3_NER_MODEL_DIR\n",
    "    modelUtil = ModelStorageUtil(model_dir)\n",
    "    nersettings = modelUtil.load_settings()\n",
    "    trainer = NerTrainer(nersettings)\n",
    "    trainer.train( training_texts, layer='gold_wordner', model_dir='test' )\n",
    "    print(f\"NerTagger treenitud {time.time() - start} sekundiga.\")\n",
    "    # Setting up the new trained nertagger and defining layers to be removed later on.\n",
    "    nertagger = NerTagger(model_dir = 'test')\n",
    "    removed_layers = ['sentences', 'morph_analysis', 'compound_tokens', 'ner', 'words', 'tokens']\n",
    "    \n",
    "    # Tagging the files using the new nertagger.\n",
    "    print(\"\\n\\nAlustan failide taggimist.\")\n",
    "    start = time.time()\n",
    "    for file in {key: value for key, value in files.items() if int(value) == subdistribution_for_testing}:\n",
    "        with open(find(file.replace(\".json\", \".txt\"), \"./vallakohtufailid/\"), 'r', encoding='UTF-8') as f:\n",
    "            text = f.read()\n",
    "            if file == \"Tartu_V6nnu_Ahja_id3502_1882a.txt\":\n",
    "                text = text.replace('..', '. .')\n",
    "            text = CompoundTokenTaggerModule.tag_text(Text(text))\n",
    "            nertagger.tag(text)\n",
    "            text.add_layer(flatten(text['ner'], 'flat_ner'))\n",
    "\n",
    "            for x in removed_layers:\n",
    "                text.pop_layer(x)\n",
    "            text_to_json(text, file=os.getcwd() + \"/vallakohtufailid_nertagger/\" + file)\n",
    "            print(f'Täägitud fail {file}')\n",
    "    print(f\"Failid taggitud {time.time() - start} sekundiga.\")\n",
    "    \n",
    "    # Chaning the tags into a readable formats for the evaluator.\n",
    "    print(\"\\n\\nAlustan tulemuste ammutamist.\")\n",
    "\n",
    "    #gold = []\n",
    "    #test = []\n",
    "    gold_ner = []\n",
    "    test_ner = []\n",
    "\n",
    "    for file in {key: value for key, value in files.items() if int(value) == subdistribution_for_testing}:\n",
    "        appendable_gold_ner = []\n",
    "        appendable_test_ner = []\n",
    "\n",
    "        if file.endswith(\".json\"):\n",
    "            if file in files_not_working:\n",
    "                continue\n",
    "            else:\n",
    "                with open(\"./vallakohtufailid_nertagger/\" + str(file), 'r', encoding='UTF-8') as f_test, \\\n",
    "                    open(\"./vallakohtufailid_json_flat/\" + str(file), 'r', encoding='UTF-8') as f_gold:\n",
    "                        test_import = json_to_text(f_test.read())\n",
    "                        gold_import = json_to_text(f_gold.read())\n",
    "\n",
    "                        # The commented part is needed for word-level-ner.\n",
    "                        '''\n",
    "                        for i in range(len(gold_import['flat_gold_wordner'])):\n",
    "                            tag = gold_import['flat_gold_wordner'][i].nertag[0]\n",
    "                            gold.append(tag)\n",
    "                        for i in range(len(test_import['flat_wordner'])):\n",
    "                            tag = test_import['flat_wordner'][i].nertag[0]\n",
    "                            test.append(tag)\n",
    "                        '''\n",
    "                        \n",
    "                        for i in range(len(gold_import['gold_ner'])):\n",
    "                            ner = gold_import['gold_ner'][i]\n",
    "                            label = ner.nertag[0]\n",
    "                            start = int(ner.start)\n",
    "                            end = int(ner.end)\n",
    "                            appendable_gold_ner.append({\"label\": label, \"start\": start, \"end\": end})\n",
    "\n",
    "                        for i in range(len(test_import['flat_ner'])):\n",
    "                            ner = test_import['flat_ner'][i]\n",
    "                            label = ner.nertag[0]\n",
    "                            start = int(ner.start)\n",
    "                            end = int(ner.end)\n",
    "                            appendable_test_ner.append({\"label\": label, \"start\": start, \"end\": end})\n",
    "\n",
    "        gold_ner.append(appendable_gold_ner)\n",
    "        test_ner.append(appendable_test_ner)\n",
    "\n",
    "    evaluator = Evaluator(gold_ner, test_ner, tags=['ORG', 'PER', 'MISC', 'LOC', 'LOC_ORG'])\n",
    "    results, results_per_tag = evaluator.evaluate()\n",
    "    all_results[subdistribution_for_testing] = (results, results_per_tag)\n",
    "print(\"Programm on lõpetanud oma töö.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_new.txt\", \"w+\") as results_file:\n",
    "    results_file.write(json.dumps(all_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_new.txt\", \"r\") as f:\n",
    "    json = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tulemused alamhulkade kaupa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_all = 0\n",
    "actual_all = 0\n",
    "possible_all = 0\n",
    "\n",
    "for i in ['1', '2', '3', '4', '5']:\n",
    "    train = []\n",
    "    for j in ['1', '2', '3', '4', '5']:\n",
    "        if j == i:\n",
    "            subdistribution_for_testing = j\n",
    "        else:\n",
    "            train.append(j)\n",
    "    print(f'Testitav alamhulk oli {subdistribution_for_testing} ning treenitavad alamhulgad {train}:')\n",
    "    correct = json[i][0]['ent_type']['correct']\n",
    "    correct_all += correct\n",
    "    actual = json[i][0]['ent_type']['actual']\n",
    "    actual_all += actual\n",
    "    possible = json[i][0]['ent_type']['possible']\n",
    "    possible_all += possible\n",
    "    precision = (correct / actual)\n",
    "    recall = (correct / possible)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1-score: {f1}\\n')\n",
    "\n",
    "\n",
    "print('Tulemused üle alamhulkade:')\n",
    "precision = correct_all / actual_all\n",
    "recall = correct_all / possible_all\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {2 * ((precision * recall) / (precision + recall))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tulemused nimeüksuste liigi kaupa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['1', '2', '3', '4', '5']:\n",
    "    train = []\n",
    "    for j in ['1', '2', '3', '4', '5']:\n",
    "        if j == i:\n",
    "            subdistribution_for_testing = j\n",
    "        else:\n",
    "            train.append(j)\n",
    "    print(f'Testitav alamhulk oli {subdistribution_for_testing} ning treenitavad alamhulgad {train}:')\n",
    "    \n",
    "    for key in list(json[i][1].keys()):\n",
    "        correct = json[i][1][str(key)]['ent_type']['correct']\n",
    "        actual = json[i][1][str(key)]['ent_type']['actual']\n",
    "        possible = json[i][1][str(key)]['ent_type']['possible']\n",
    "        precision = (correct / actual)\n",
    "        recall = (correct / possible)\n",
    "        f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "        print(f'Key: {key}')\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'F1-score: {f1}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

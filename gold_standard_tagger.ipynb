{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from estnltk import EnvelopingBaseSpan\n",
    "from estnltk import Text, Layer, Annotation, EnvelopingSpan, Span\n",
    "from estnltk.converters import text_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Harju_Hageri_Kohila_id10509_1871a.txt: 8 13\n",
      "2. Harju_Hageri_Kohila_id1346_1888a.txt: 7 10\n",
      "3. Harju_Hageri_Kohila_id20604_1883a.txt: 5 10\n",
      "4. Harju_Hageri_Kohila_id21431_1885a.txt: 5 11\n",
      "5. Harju_Hageri_Kohila_id23811_1873a.txt: 20 34\n",
      "6. Harju_Hageri_Kohila_id3108_1885a.txt: 7 8\n",
      "E. Jb. Rändith\n",
      "E. Jb. Rändith\n",
      "7. Harju_Hageri_Kohila_id4010_1890a.txt: 47 77\n",
      "8. Harju_Hageri_Kohila_id4177_1883a.txt: 29 43\n",
      "9. Harju_J6el2htme_J6el2htme_id7364_1868a.txt: 7 9\n",
      "10. Harju_J6el2htme_J6el2htme_id7659_1870a.txt: 21 28\n",
      "11. Harju_Juuru_Juuru_id19451_1886a.txt: 11 14\n",
      "12. Harju_Juuru_Juuru_id19472_1887a.txt: 16 24\n",
      "13. Harju_Juuru_Juuru_id23775_1873a.txt: 12 25\n",
      "14. Harju_Juuru_Juuru_id556_1877a.txt: 5 9\n",
      "15. Harju_Juuru_Kaiu_id12588_1883a.txt: 9 15\n",
      "16. Harju_Juuru_Kaiu_id18571_1873a.txt: 10 18\n",
      "17. Harju_Juuru_Kaiu_id3479_1886a.txt: 10 15\n",
      "18. Harju_Juuru_Kaiu_id9788_1882a.txt: 11 16\n",
      "19. Harju_Jyri_Rae_id3658_1888a.txt: 8 10\n",
      "20. Harju_Keila_Saue_id14410_1886a.txt: 4 10\n",
      "21. Harju_Kose_Kose-Uuem6isa_id3340_1868a.txt: 9 20\n",
      "22. Harju_Kose_Palvere_id20647_1885a.txt: 19 22\n",
      "23. Harju_Kose_Palvere_id23127_1887a.txt: 9 13\n",
      "24. Harju_Kose_Triigi_id21084_1874a.txt: 9 16\n",
      "25. Harju_Kose_Triigi_id9421_1867a.txt: 11 26\n",
      "26. Harju_Kose_Triigi_id9684_1869a.txt: 8 19\n",
      "27. Harju_Kose_Triigi_id9764_1869a.txt: 6 13\n",
      "28. Harju_Kuusalu_Kolga_id11586_1887a.txt: 9 17\n",
      "29. Harju_Kuusalu_Kolga_id11722_1887a.txt: 13 19\n",
      "30. Harju_Kuusalu_Kolga_id12078_1888a.txt: 14 19\n",
      "31. Harju_Rapla_Rapla_id20935_1870a.txt: 7 12\n",
      "32. J2rva_Ambla_Ambla_id11918_1883a.txt: 4 7\n",
      "33. J2rva_Ambla_Ambla_id5939_1888a.txt: 9 28\n",
      "34. J2rva_Ambla_Ambla_id6188_1889a.txt: 12 27\n",
      "35. J2rva_Ambla_Ambla_id6216_1889a.txt: 7 10\n",
      "36. J2rva_Ambla_Uudekyla_id13485_1867a.txt: 16 40\n",
      "37. J2rva_Anna_Purdi_id18906_1870a.txt: 8 20\n",
      "38. J2rva_Anna_Purdi_id23132_1870a.txt: 8 10\n",
      "39. J2rva_Peetri_Silmsi_id23900_1869a.txt: 9 14\n",
      "40. J2rva_Peetri_V2ike-Kareda_id19110_1867a.txt: 5 6\n",
      "41. J2rva_Peetri_V2ike-Kareda_id19589_1871a.txt: 8 16\n",
      "42. J2rva_Tyri_Kirna_id23402_1872a.txt: 3 17\n",
      "43. J2rva_Tyri_Kirna_id23696_1873a.txt: 6 21\n",
      "44. J2rva_Tyri_Kirna_id23791_1874a.txt: 4 14\n",
      "45. J2rva_Tyri_Kirna_id23886_1875a.txt: 7 19\n",
      "46. J2rva_Tyri_Kirna_id24911_1881a.txt: 8 27\n",
      "47. J2rva_Tyri_Kirna_id24973_1881a.txt: 6 19\n",
      "48. J2rva_Tyri_Kirna_id25452_1882a.txt: 7 19\n",
      "49. J2rva_Tyri_S2revere_id12869_1879a.txt: 5 12\n",
      "50. J2rva_Tyri_S2revere_id13938_1885a.txt: 5 16\n",
      "51. J2rva_Tyri_S2revere_id13997_1886a.txt: 5 12\n",
      "52. J2rva_Tyri_S2revere_id14526_1886a.txt: 4 6\n",
      "53. J2rva_Tyri_S2revere_id5550_1881a.txt: 7 19\n",
      "54. J2rva_Tyri_S2revere_id8279_1885a.txt: 15 23\n",
      "55. J2rva_Tyri_S2revere_id8880_1886a.txt: 23 42\n",
      "56. J2rva_Tyri_S2revere_id9101_1882a.txt: 6 26\n",
      "57. J2rva_Tyri_Tyri-Alliku_id1370_1894a.txt: 7 12\n",
      "58. J2rva_Tyri_Tyri-Alliku_id2315_1897a.txt: 13 24\n",
      "59. J2rva_Tyri_V22tsa_id16836_1886a.txt: 8 28\n",
      "60. J2rva_Tyri_V22tsa_id16955_1887a.txt: 7 16\n",
      "61. J2rva_Tyri_V22tsa_id17663_1888a.txt: 7 17\n",
      "62. J2rva_Tyri_V22tsa_id20769_1903a.txt: 14 21\n",
      "63. J2rva_Tyri_V22tsa_id22259_1913a.txt: 9 22\n",
      "64. L22ne_Emmaste_Emmaste_id17976_1898a.txt: 12 13\n",
      "65. L22ne_K2ina_Putkaste_id8765_1867a.txt: 23 25\n",
      "66. L22ne_Kullamaa_Kuij6e_id15112_1868a.txt: 3 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-96e90af86032>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgold_wordner_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mtext_to_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcwd\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\\\vallakohtufailid_json\\\\\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\".json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Kaust {directory} on läbitud.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Programm on lõpetanud oma töö.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py37\\lib\\site-packages\\estnltk\\converters\\json_exporter.py\u001b[0m in \u001b[0;36mtext_to_json\u001b[1;34m(text, file, file_encoding)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_encoding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mout_f\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py37\\lib\\codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[0mremembers\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mcalls\u001b[0m \u001b[0mto\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \"\"\"\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m         \"\"\"\n\u001b[0;32m    188\u001b[0m         \u001b[0mCreates\u001b[0m \u001b[0man\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# If the annotation contains newline character, then indexes will contain ';' at the linebreak (e.g. 388 393;394 398 )\n",
    "indexes_on_line_split = re.compile(r' (\\d+) (\\d+;\\d+ ){1,}(\\d+)$')\n",
    "\n",
    "def collect_annotations( in_f ):\n",
    "    annotations = []\n",
    "    split_lines_ahead = 0\n",
    "    for line in in_f:\n",
    "        line = line.rstrip('\\n')\n",
    "        items = line.split('\\t')\n",
    "        if split_lines_ahead > 0:\n",
    "            split_lines_ahead -= 1\n",
    "            last_item = annotations[-1]\n",
    "            new_tuple = (last_item[0],last_item[1],last_item[2],last_item[3]+line,last_item[4])\n",
    "            annotations[-1] = new_tuple\n",
    "            continue\n",
    "        if len(items) == 3:\n",
    "            indexes_str = items[1]\n",
    "            if indexes_str.count(';') > 0:\n",
    "                split_lines_ahead += indexes_str.count(';')\n",
    "            indexes_str = indexes_on_line_split.sub(' \\\\1 \\\\3', indexes_str)\n",
    "            tag, start, end = indexes_str.split()\n",
    "            annotations.append( (tag, start, end, items[2], items[0]) )\n",
    "    seen = set()\n",
    "    removed_duplicates_annotations = []\n",
    "    for a, b, c, d, e in annotations:\n",
    "        if not b in seen:\n",
    "            seen.add(b)\n",
    "            removed_duplicates_annotations.append((a, b, c, d, e))\n",
    "        else:\n",
    "            for index, item in enumerate(removed_duplicates_annotations):\n",
    "                if item[1] == b and item[2] > c:\n",
    "                    tuple_without_n = (a, b, c, d, e)\n",
    "                    item = tuple_without_n\n",
    "                    removed_duplicates_annotations[index] = item\n",
    "                elif item[1] == b and item[2] < c:\n",
    "                    tuple_without_n = (a, b, item[2], d, e)\n",
    "                    item = tuple_without_n\n",
    "                    removed_duplicates_annotations[index] = item\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "    annotations = sorted(list(set(removed_duplicates_annotations)), key=lambda x: int(x[1]))\n",
    "    return annotations\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "rownr = 1\n",
    "directories = [\"vallakohus_esimene\", \"vallakohus_teine\", \"vallakohus_kolmas\", \"vallakohus_neljas\"]\n",
    "for directory in directories:\n",
    "    path = cwd + \"\\\\\" + directory + \"\\\\\"\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(path + file, 'r', encoding=\"utf-8\") as txt, open(path + file.split(\".\")[0] + \".ann\", 'r', encoding=\"utf-8\") as ann:\n",
    "                textfile = txt.read().replace(u\"\\xa0\", u\" \")\n",
    "\n",
    "                # converting the text form .txt file into an EstNLTK Text object and giving it the \"words\" layer\n",
    "                text = Text(textfile)\n",
    "                text.meta['origin_directory'] = str(directory)\n",
    "                text = text.tag_layer(['words'])\n",
    "\n",
    "                gold_ner_layer = Layer(name=\"gold_ner\", text_object=text, attributes=['nertag'])\n",
    "                gold_wordner_layer = Layer(name=\"gold_wordner\", text_object=text, attributes=['nertag'], parent=\"words\")\n",
    "\n",
    "                fixed_annotations = collect_annotations(ann)\n",
    "\n",
    "                annotation_dictionary = {}\n",
    "                for annotation in fixed_annotations:\n",
    "                    trigger = annotation[4]\n",
    "                    location = annotation[0] + \" \" + annotation[1] + \" \" + annotation[2]\n",
    "                    entity = annotation[3]\n",
    "                    annotation_dictionary[trigger] = [location, entity]\n",
    "\n",
    "                for key in annotation_dictionary:\n",
    "                    name = []\n",
    "                    value = annotation_dictionary.get(key)\n",
    "                    \n",
    "                    entity = value[1]\n",
    "                    ner, startIndex, endIndex = value[0].split(\" \")\n",
    "                    \n",
    "                    if entity.count(\".\") > 1: #TODO: SIIN ON PROBLEEM\n",
    "                        entity.replace(\".\", \" . \")\n",
    "\n",
    "                    #TODO: Probleem on siin: ma ei saa kontrollida preceding_newlinese kuidagi, vaid ma pean programmi ümber kirjutama nii, et mitte if tekstisõna index on sama, mis algusindeks, vaid\n",
    "                    #tekstisõna = annotationi sõna algusega ja siis arvutama preceding_newlinese. Praegu on .ann failis näiteks algusindeks 705, kui ma loen enne seda kokku 46 preceding newline'i sellest\n",
    "                    #tekstifailist, siis neid on liiga palju, sest teksti enda indeksid on juba teistsugused...\n",
    "                    \n",
    "                    \n",
    "                    for i in range(0, len(text.words)):\n",
    "                        if text.words[i].text == entity.split(\" \")[0]:\n",
    "                            \n",
    "                            preceding_newlines = text.text[:int(text.words[i].start)].count(\"\\n\")\n",
    "                            startIndex = int(startIndex) - int(preceding_newlines)\n",
    "                            endIndex = int(endIndex) - int(preceding_newlines)\n",
    "                            \n",
    "                            if text.words[i].start == startIndex:\n",
    "                                if text.words[i].end == endIndex:\n",
    "                                    base_span = EnvelopingBaseSpan([text.words[i].base_span])\n",
    "                                    name = [text.words[i]] #adding the name element(s) to a list to use in the wordner dictionary later on\n",
    "                                else:\n",
    "                                    if text.words[i+1].end == endIndex: #if the next word's endindex is the same as the endindex in the .ann file, add a new span\n",
    "                                        name = [text.words[i], text.words[i+1]]\n",
    "                                    else:\n",
    "                                        for j in range(len(entity.split(\" \"))):\n",
    "                                            name.append(text.words[i+j])\n",
    "                                    base_span = EnvelopingBaseSpan([s.base_span for s in name])\n",
    "                                new_span = EnvelopingSpan(base_span, layer=gold_ner_layer)\n",
    "\n",
    "                                dictionary_for_wordner = dict()\n",
    "                                if ner == \"Isik\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"PER\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-PER\" \n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-PER\"\n",
    "                                if ner == \"KO_koht\" or ner == \"KO_org\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"LOC_ORG\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-LOC_ORG\"\n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-LOC_ORG\"\n",
    "                                if ner == \"Koht\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"LOC\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-LOC\"\n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-LOC\"\n",
    "                                if ner == \"Org\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"ORG\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-ORG\" \n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-ORG\"\n",
    "                                if ner == \"Muu\" or ner == \"Teadmata\" or ner == \"ese\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"MISC\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-MISC\" \n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-MISC\"\n",
    "                                gold_ner_layer.add_span(new_span)\n",
    "                                break\n",
    "                text.add_layer(gold_ner_layer)\n",
    "                if len(text.gold_ner) != len(fixed_annotations):\n",
    "                    print(str(rownr) + \". \" + str(file) + \":\", len(text.gold_ner), len(fixed_annotations))\n",
    "                    rownr += 1\n",
    "                for i in range(0, len(text.words)):\n",
    "                    for key in dictionary_for_wordner.keys():\n",
    "                        new_span = Span(base_span=text.words[i].base_span, layer=gold_wordner_layer)\n",
    "\n",
    "                        if i == key:\n",
    "                            new_span.add_annotation(Annotation(new_span, nertag=str(dictionary_for_wordner.get(key))))\n",
    "                            gold_wordner_layer.add_span(new_span)\n",
    "                            break\n",
    "                        else:\n",
    "                            if i in dictionary_for_wordner.keys():\n",
    "                                continue\n",
    "                            else:\n",
    "                                new_span.add_annotation(Annotation(new_span, nertag=\"O\"))\n",
    "                            gold_wordner_layer.add_span(new_span)\n",
    "                            break\n",
    "                \n",
    "                text.add_layer(gold_wordner_layer)\n",
    "                text_to_json(text, file=cwd + \"\\\\vallakohtufailid_json\\\\\" + file.replace(\".txt\", \".json\"))\n",
    "    print(f\"Kaust {directory} on läbitud.\")\n",
    "print(\"Programm on lõpetanud oma töö.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

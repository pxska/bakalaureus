{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(li1, li2):\n",
    "    return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from estnltk import EnvelopingBaseSpan\n",
    "from estnltk import Text, Layer, Annotation, EnvelopingSpan, Span\n",
    "from estnltk.converters import text_to_json\n",
    "from estnltk.layer_operations import extract_sections\n",
    "from estnltk.taggers import Retagger\n",
    "from estnltk.taggers import CompoundTokenTagger\n",
    "\n",
    "class TokenSplitter( Retagger ):\n",
    "    \"\"\"Splits tokens into smaller tokens based on regular expression patterns.\"\"\" \n",
    "    conf_param = ['patterns', 'break_group_name']\n",
    "    \n",
    "    def __init__(self, patterns, break_group_name:str='end'):\n",
    "        # Set input/output layers\n",
    "        self.input_layers = ['tokens']\n",
    "        self.output_layer = 'tokens'\n",
    "        self.output_attributes = ()\n",
    "        # Set other configuration parameters\n",
    "        if not (isinstance(break_group_name, str) and len(break_group_name) > 0):\n",
    "            raise TypeError('(!) break_group_name should be a non-empty string.')\n",
    "        self.break_group_name = break_group_name\n",
    "        # Assert that all patterns are regular expressions in the valid format\n",
    "        if not isinstance(patterns, list):\n",
    "            raise TypeError('(!) patterns should be a list of compiled regular expressions.')\n",
    "        # TODO: we use an adhoc way to verify that patterns are regular expressions \n",
    "        #       because there seems to be no common way of doing it both in py35 \n",
    "        #       and py36\n",
    "        for pat in patterns:\n",
    "            # Check for the existence of methods/attributes\n",
    "            has_match   = callable(getattr(pat, \"match\", None))\n",
    "            has_search  = callable(getattr(pat, \"search\", None))\n",
    "            has_pattern = getattr(pat, \"pattern\", None) is not None\n",
    "            for (k,v) in (('method match()',has_match),\\\n",
    "                          ('method search()',has_search),\\\n",
    "                          ('attribute pattern',has_pattern)):\n",
    "                if v is False:\n",
    "                    raise TypeError('(!) Unexpected regex pattern: {!r} is missing {}.'.format(pat, k))\n",
    "            symbolic_groups = pat.groupindex\n",
    "            if self.break_group_name not in symbolic_groups.keys():\n",
    "                raise TypeError('(!) Pattern {!r} is missing symbolic group named {!r}.'.format(pat, self.break_group_name))\n",
    "        self.patterns = patterns\n",
    "\n",
    "    def _change_layer(self, text, layers, status):\n",
    "        # Get changeble layer\n",
    "        changeble_layer = layers[self.output_layer]\n",
    "        # Iterate over tokens\n",
    "        add_spans    = []\n",
    "        remove_spans = []\n",
    "        for span in changeble_layer:\n",
    "            token_str = text.text[span.start:span.end]\n",
    "            for pat in self.patterns:\n",
    "                m = pat.search(token_str)\n",
    "                if m:\n",
    "                    break_group_end = m.end( self.break_group_name )\n",
    "                    if break_group_end > -1 and \\\n",
    "                       break_group_end > 0  and \\\n",
    "                       break_group_end < len(token_str):\n",
    "                        # Make the split\n",
    "                        add_spans.append( (span.start, span.start+break_group_end) )\n",
    "                        add_spans.append( (span.start+break_group_end, span.end) )\n",
    "                        remove_spans.append( span )\n",
    "                        # Once a token has been split, then break and move on to \n",
    "                        # the next token ...\n",
    "                        break\n",
    "        if add_spans:\n",
    "            assert len(remove_spans) > 0\n",
    "            for old_span in remove_spans:\n",
    "                changeble_layer.remove_span( old_span )\n",
    "            for new_span in add_spans:\n",
    "                changeble_layer.add_annotation( new_span )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Probleem failis Tartu_Kodavere_Pala_id23275_1872a.txt, mille erinevus on: [' Annuka asse']\n",
      "2. Probleem failis Viljandi_Paistu_Holstre_id9042_1836a.txt, mille erinevus on: ['Jaan Park_']\n",
      "3. Probleem failis Tartu_Kodavere_Alatskivi_id7763_1879a.txt, mille erinevus on: ['A. Pärn']\n",
      "4. Probleem failis Harju_Hageri_Kohila_id4177_1883a.txt, mille erinevus on: ['Pridik Post', 'Pridik Post-']\n",
      "5. Probleem failis Harju_Keila_Saue_id14410_1886a.txt, mille erinevus on: ['E v ZurMühlen', 'E v Zur']\n",
      "6. Probleem failis Tartu_Kodavere_Pala_id22870_1872a.txt, mille erinevus on: ['Jaan\\nAnni']\n",
      "7. Probleem failis Tartu_Kodavere_Alatskivi_id11023_1880a.txt, mille erinevus on: ['A. Pärn']\n",
      "Kaust vallakohus_esimene on läbitud.\n",
      "8. Probleem failis Tartu_Kodavere_Pala_id17236_1857a.txt, mille erinevus on: ['Jahn Kuk', 'Jahn  Kuk']\n",
      "9. Probleem failis Tartu_Kodavere_Pala_id20447_1867a.txt, mille erinevus on: ['Jaan  Etro']\n",
      "10. Probleem failis P2rnu_Audru_V6lla_id5372_1878a.txt, mille erinevus on: ['Kadri Puusid', 'Kadri  Puusid']\n",
      "11. Probleem failis Harju_Hageri_Kohila_id11667_1879a.txt, mille erinevus on: ['Angerja ', 'Angerja']\n",
      "12. Probleem failis J2rva_Tyri_Kirna_id24596_1880a.txt, mille erinevus on: ['Weatsa walla', 'Weatsa  walla']\n",
      "13. Probleem failis Tartu_Kodavere_Pala_id20260_1866a.txt, mille erinevus on: ['Maddis  Sarwik', 'Märt  Kukk', 'Maddis\\n\\nSarwik', 'Märt\\n\\nKukk']\n",
      "14. Probleem failis Tartu_Kodavere_Alatskivi_id15625_1876a.txt, mille erinevus on: ['Kökkora -Kallastelt', 'Kökkora -']\n",
      "15. Probleem failis L22ne_Kullamaa_Kuij6e_id15780_1889a.txt, mille erinevus on: ['J. Laan', 'J']\n",
      "16. Probleem failis Viljandi_Paistu_Holstre_id11341_1848a.txt, mille erinevus on: ['Kaiserlikko Willandi silla kohto', 'Kaiserlikko  Willandi silla kohto']\n",
      "17. Probleem failis Tartu_Kodavere_Pala_id22839_1872a.txt, mille erinevus on: ['Elias Kubjas', 'Elias  Kubjas']\n",
      "18. Probleem failis Saare_Kaarma_Loona_id7599_1910a.txt, mille erinevus on: ['Nikolai Jakobson']\n",
      "19. Probleem failis V6ru_Vastseliina_Misso_id13326_1887a.txt, mille erinevus on: ['Jaan Oppar', 'Jaan  Oppar']\n",
      "20. Probleem failis Tartu_Torma_Avinurme_id17064_1870a.txt, mille erinevus on: ['Adrako N 16 tallo', 'Adrako N 16 tallo mis', 'Adrako N 16 tallo perremees']\n",
      "21. Probleem failis Tartu_Kodavere_Alatskivi_id15341_1876a.txt, mille erinevus on: ['Aleksai Perdsitki', 'Aleksai  Perdsitki']\n",
      "22. Probleem failis Tartu_Kodavere_Pala_id23317_1872a.txt, mille erinevus on: ['Märt Piiri', 'Märt      Piiri']\n",
      "Kaust vallakohus_teine on läbitud.\n",
      "23. Probleem failis Viljandi_Viljandi_Karula_id19401_1868a.txt, mille erinevus on: ['Jaan  Annusele', 'Jaan\\n\\nAnnusele']\n",
      "24. Probleem failis J2rva_Tyri_V22tsa_id22092_1912a.txt, mille erinevus on: ['Lõõla  küla']\n",
      "25. Probleem failis Harju_Keila_Harku_id14723_1869a.txt, mille erinevus on: ['Mart', 'Mart-']\n",
      "26. Probleem failis Tartu_Kodavere_Pala_id21737_1870a.txt, mille erinevus on: ['Jürri Priks', 'Jürri  Priks']\n",
      "27. Probleem failis V6ru_R6uge_Saaluse_id10962_1879a.txt, mille erinevus on: ['Jaan  Aija']\n",
      "28. Probleem failis V6ru_R2pina_Kahkva_id6489_1887a.txt, mille erinevus on: ['Peter Rämman K 14']\n",
      "29. Probleem failis Harju_Rapla_Rapla_id20943_1870a.txt, mille erinevus on: ['Kappeta ', 'Kappeta']\n",
      "30. Probleem failis J2rva_J2rva-Jaani_Einmanni_id9824_1868a.txt, mille erinevus on: ['Jakobi ', 'Mä ']\n",
      "31. Probleem failis Viru_Rakvere_S6meru_id10293_1883a.txt, mille erinevus on: ['Aresilt ']\n",
      "32. Probleem failis Harju_Kose_Palvere_id16729_1881a.txt, mille erinevus on: ['Tõnno  Gruno']\n",
      "33. Probleem failis Viljandi_Paistu_Holstre_id8697_1835a.txt, mille erinevus on: ['Paisto -kirriko-moisas', 'Paisto -']\n",
      "34. Probleem failis J2rva_Tyri_S2revere_id8113_1885a.txt, mille erinevus on: ['Lokkotalt ']\n",
      "35. Probleem failis V6ru_Kanepi_Krootuse_id24412_1885a.txt, mille erinevus on: ['Peter Likert', 'Peter  Likert']\n",
      "36. Probleem failis Harju_Kose_Habaja_id675_1874a.txt, mille erinevus on: ['Klaukse Maja', 'Klaukse  Maja']\n",
      "37. Probleem failis Tartu_Kodavere_Alatskivi_id6169_1879a.txt, mille erinevus on: ['A. Pärn']\n",
      "38. Probleem failis Tartu_Kodavere_Pala_id22941_1872a.txt, mille erinevus on: ['Märt Piiri', 'Elias Kubjas', 'Märt      Piiri', 'Elias      Kubjas']\n",
      "39. Probleem failis Tartu_Kodavere_Pala_id22626_1871a.txt, mille erinevus on: ['Josep Sallo', 'Märt Piiri', 'Märt     Piiri', 'Josep  Sallo']\n",
      "40. Probleem failis Tartu_Kodavere_Pala_id21272_1869a.txt, mille erinevus on: ['Josep Hawaki']\n",
      "41. Probleem failis J2rva_Tyri_S2revere_id12373_1877a.txt, mille erinevus on: ['Hinrik Kotsar', 'Hinrik  Kotsar']\n",
      "42. Probleem failis Viljandi_P6ltsamaa_Adavere_id17425_1894a.txt, mille erinevus on: ['Leping  Astus kohtu ette Mihkel Kirschberg', 'Leping\\n\\nAstus kohtu ette Mihkel Kirschberg']\n",
      "43. Probleem failis J2rva_Tyri_Kirna_id24064_1879a.txt, mille erinevus on: ['Toomas  Kruup']\n",
      "Kaust vallakohus_kolmas on läbitud.\n",
      "44. Probleem failis Tartu_Kodavere_Pala_id17804_1861a.txt, mille erinevus on: ['Laur Olla', 'Laur  Olla']\n",
      "45. Probleem failis Tartu_Laiuse_Kivij2rve_id13164_1866a.txt, mille erinevus on: [' Thomas Peterson']\n",
      "46. Probleem failis Tartu_Kodavere_Pala_id22811_1872a.txt, mille erinevus on: ['Märt Piiri', 'Märt      Piiri']\n",
      "47. Probleem failis Tartu_V6nnu_Ahja_id22345_1868a.txt, mille erinevus on: ['Roma ', 'Roma']\n",
      "48. Probleem failis J2rva_Tyri_Kirna_id24928_1881a.txt, mille erinevus on: ['Jaan  Neewe']\n",
      "49. Probleem failis Harju_Kose_Palvere_id17074_1882a.txt, mille erinevus on: ['Jüri Ummus', 'Jüri   Ummus']\n",
      "50. Probleem failis Tartu_Kodavere_Pala_id22058_1871a.txt, mille erinevus on: ['Gustav  Waddi', 'Gustav\\n\\nWaddi']\n",
      "51. Probleem failis Tartu_Kodavere_Pala_id22036_1870a.txt, mille erinevus on: ['Karel Rosenberg', 'Jürri Briks', 'Jürri   Briks', 'Karel    Rosenberg']\n",
      "52. Probleem failis Tartu_V6nnu_Ahja_id23568_1895a.txt, mille erinevus on: ['J Kooskord\\u200b\\u200b\\u200b\\u200b', 'J Kooskord\\u200b\\u200b\\u200b\\u200b &lt;']\n",
      "53. Probleem failis Tartu_Kodavere_Pala_id18894_1865a.txt, mille erinevus on: ['Jaak  Tuhha', 'Jaan     Kukk', 'Jaak\\n\\nTuhha']\n",
      "54. Probleem failis Tartu_Kodavere_Alatskivi_id14511_1876a.txt, mille erinevus on: ['Tartu -lina', 'Tartu -', 'Kasepää küla-']\n",
      "55. Probleem failis Tartu_Kodavere_Pala_id16229_1849a.txt, mille erinevus on: ['Anno Sirk', 'Anno     Sirk']\n",
      "56. Probleem failis Tartu_Kodavere_Pala_id16159_1849a.txt, mille erinevus on: ['Tõnno Mölderi', 'Tõnno  Mölderi']\n",
      "57. Probleem failis Tartu_Kodavere_Alatskivi_id12579_1876a.txt, mille erinevus on: ['Laheperra', 'Laheperra-sepp']\n",
      "58. Probleem failis J2rva_Tyri_V22tsa_id18360_1888a.txt, mille erinevus on: ['Kristoff  Willberg', 'Aleksandri Suttingile', 'Kristoff\\n\\nWillberg', 'Aleksandri  Suttingile']\n",
      "59. Probleem failis Tartu_Kodavere_Pala_id25066_1873a.txt, mille erinevus on: ['Elias Kubja', 'Märt  Piiri', 'Märt    Piiri', 'Elias    Kubja', 'Märt\\n\\nPiiri']\n",
      "60. Probleem failis Tartu_V6nnu_Ahja_id13367_1881a.txt, mille erinevus on: ['Ahka KogoKonna Kohtu', 'Ahka KogoKonna']\n",
      "61. Probleem failis Tartu_Kodavere_Pala_id22898_1872a.txt, mille erinevus on: ['Karel Saarel', 'Karel  Saarel']\n",
      "62. Probleem failis Tartu_Kodavere_Alatskivi_id11390_1880a.txt, mille erinevus on: ['Tallorahwa seaduse ramato', 'Tallorahwa seaduse  ramato']\n",
      "63. Probleem failis Tartu_Kodavere_Pala_id22108_1871a.txt, mille erinevus on: ['Märt Piiri', 'Märt     Piiri']\n",
      "64. Probleem failis Tartu_Kodavere_Alatskivi_id7573_1879a.txt, mille erinevus on: ['Jegor AmeljanowPersitski']\n",
      "65. Probleem failis Tartu_Kodavere_Alatskivi_id15715_1877a.txt, mille erinevus on: ['Sawi metsa', 'Sawi metsa-']\n",
      "66. Probleem failis Tartu_Kodavere_Alatskivi_id13497_1881a.txt, mille erinevus on: ['Tallorahwa seaduse ramato', 'Tallorahwa seaduse  ramato']\n",
      "67. Probleem failis Tartu_V6nnu_Ahja_id3502_1882a.txt, mille erinevus on: ['T. S. R.', 'T. S. R..']\n",
      "68. Probleem failis Tartu_Kodavere_Pala_id21335_1869a.txt, mille erinevus on: ['Jaan Anni', 'Jaan \\n\\nAnni']\n",
      "Kaust vallakohus_neljas on läbitud.\n",
      "Programm on lõpetanud oma töö.\n"
     ]
    }
   ],
   "source": [
    "# If the annotation contains newline character, then indexes will contain ';' at the linebreak (e.g. 388 393;394 398 )\n",
    "indexes_on_line_split = re.compile(r' (\\d+) (\\d+;\\d+ ){1,}(\\d+)$')\n",
    "\n",
    "def collect_annotations( in_f ):\n",
    "    annotations = []\n",
    "    split_lines_ahead = 0\n",
    "    for line in in_f:\n",
    "        line = line.rstrip('\\n')\n",
    "        items = line.split('\\t')\n",
    "        if split_lines_ahead > 0:\n",
    "            split_lines_ahead -= 1\n",
    "            last_item = annotations[-1]\n",
    "            new_tuple = (last_item[0],last_item[1],last_item[2],(last_item[3]+line),last_item[4])\n",
    "            annotations[-1] = new_tuple\n",
    "            continue\n",
    "        if len(items) == 3:\n",
    "            indexes_str = items[1]\n",
    "            if indexes_str.count(';') > 0:\n",
    "                split_lines_ahead += indexes_str.count(';')\n",
    "            indexes_str = indexes_on_line_split.sub(' \\\\1 \\\\3', indexes_str)\n",
    "            tag, start, end = indexes_str.split()\n",
    "            annotations.append( (tag, start, end, items[2], items[0]) )\n",
    "    seen = set()\n",
    "    removed_duplicates_annotations = []\n",
    "    for a, b, c, d, e in annotations:\n",
    "        if not b in seen:\n",
    "            seen.add(b)\n",
    "            removed_duplicates_annotations.append((a, b, c, d, e))\n",
    "        else:\n",
    "            for index, item in enumerate(removed_duplicates_annotations):\n",
    "                if item[1] == b and item[2] > c:\n",
    "                    tuple_without_n = (a, b, c, d, e)\n",
    "                    item = tuple_without_n\n",
    "                    removed_duplicates_annotations[index] = item\n",
    "                elif item[1] == b and item[2] < c:\n",
    "                    tuple_without_n = (a, b, item[2], d, e)\n",
    "                    item = tuple_without_n\n",
    "                    removed_duplicates_annotations[index] = item\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "    for index, item in enumerate(removed_duplicates_annotations):\n",
    "        if \"\\xa0\" in item[3]:\n",
    "            replaced = re.sub(r'\\s\\s+', r' ', item[3].replace(u'\\xa0', u' '))\n",
    "            removed_duplicates_annotations[index] = ( item[0], item[1], item[2], replaced, item[4] )\n",
    "    \n",
    "    annotations = sorted(list(set(removed_duplicates_annotations)), key=lambda x: int(x[1]))\n",
    "    return annotations\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "token_splitter = TokenSplitter(patterns=[re.compile(r'(?P<end>[A-ZÕÄÖÜ]{1}\\w+)[A-ZÕÄÖÜ]{1}\\w+'),\\\n",
    "                                         re.compile(r'(?P<end>Piebenomme)metsawaht'),\\\n",
    "                                         re.compile(r'(?P<end>maa)peal'),\\\n",
    "                                         re.compile(r'(?P<end>reppi)käest'),\\\n",
    "                                         re.compile(r'(?P<end>-)Kallastelt'),\\\n",
    "                                         re.compile(r'(?P<end>Kiidjerwelt)J'),\\\n",
    "                                         re.compile(r'(?P<end>Ameljanow)Persitski'),\\\n",
    "                                         re.compile(r'(?P<end>mõistmas)Mihkel'),\\\n",
    "                                         re.compile(r'(?P<end>tema)Käkk'),\\\n",
    "                                         re.compile(r'(?P<end>Ahjawalla)liikmed'),\\\n",
    "                                         re.compile(r'(?P<end>kohtumees)A\\.'),\\\n",
    "                                         re.compile(r'(?P<end>Pechmann)x'),\\\n",
    "                                         re.compile(r'(?P<end>pölli)Anni'),\\\n",
    "                                         re.compile(r'(?P<end>külla)Rauba'),\\\n",
    "                                         re.compile(r'(?P<end>kohtowannem)Jaak'),\\\n",
    "                                         re.compile(r'(?P<end>rannast)Leno'),\\\n",
    "                                         re.compile(r'(?P<end>wallast)Kiiwita'),\\\n",
    "                                         re.compile(r'(?P<end>wallas)Kristjan'),\\\n",
    "                                         re.compile(r'(?P<end>Pedoson)rahul'),\\\n",
    "                                         re.compile(r'(?P<end>pere)Jaan'),\\\n",
    "                                         re.compile(r'(?P<end>kohtu)poolest'),\\\n",
    "                                         re.compile(r'(?P<end>Kurrista)kaudo'),\\\n",
    "                                         re.compile(r'(?P<end>mölder)Gottlieb'),\\\n",
    "                                         re.compile(r'(?P<end>wöörmündri)Jaan'),\\\n",
    "                                         re.compile(r'(?P<end>Oinas)ja'),\\\n",
    "                                         re.compile(r'(?P<end>ette)Leenu'),\\\n",
    "                                         re.compile(r'(?P<end>Tommingas)peab'),\\\n",
    "                                         re.compile(r'(?P<end>wäljaja)Kotlep')])\n",
    "rownr = 1\n",
    "directories = [\"vallakohus_esimene\", \"vallakohus_teine\", \"vallakohus_kolmas\", \"vallakohus_neljas\"]\n",
    "for directory in directories:\n",
    "    path = cwd + \"/\" + directory + \"/\"\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(path + file, 'r', encoding=\"utf-8\") as txt, open(path + file.split(\".\")[0] + \".ann\", 'r', encoding=\"utf-8\") as ann:\n",
    "                textfile = txt.read().replace(u'\\xa0', u' ')\n",
    "                dictionary_for_wordner = dict()\n",
    "                # converting the text form .txt file into an EstNLTK Text object and giving it the \"words\" layer\n",
    "                text = Text(textfile)\n",
    "                text.meta['origin_directory'] = str(directory)\n",
    "                text = text.tag_layer(['tokens'])\n",
    "                token_splitter.retag(text)\n",
    "                CompoundTokenTagger(tag_initials = False).tag(text)\n",
    "                text = text.tag_layer(['words'])\n",
    "                \n",
    "                \n",
    "                # creating NER layers\n",
    "                gold_ner_layer = Layer(name=\"gold_ner\", text_object=text, attributes=['nertag'])\n",
    "                gold_wordner_layer = Layer(name=\"gold_wordner\", text_object=text, attributes=['nertag'], parent=\"words\")\n",
    "                \n",
    "                #fixing annotations \n",
    "                fixed_annotations = collect_annotations(ann)\n",
    "\n",
    "                annotation_dictionary = {}\n",
    "                for annotation in fixed_annotations:\n",
    "                    trigger = annotation[4]\n",
    "                    location = annotation[0] + \" \" + annotation[1] + \" \" + annotation[2]\n",
    "                    entity = annotation[3]\n",
    "                    annotation_dictionary[trigger] = [location, entity]\n",
    "\n",
    "                for key in annotation_dictionary:\n",
    "                    name = []\n",
    "                    no_split_at_tick = [\"J. Paap'le\", \"J. Särg'e\", \"Peter Issakson's Wermogen\", \"Aleksei Jaani p. Tõnsau'le\", \"J. Grünberg'ile\",\\\n",
    "                                        \"Magnus v. Freymann'ile\", \"J. Sõber'ile\", \"J. Kukers'il\", \"Peter Issakson's Wermogen\", \"A.Lilja´le\",\\\n",
    "                                        \"Juhan Mihkli p Kaasik´u\", \"Foma Kirilli p Robert´ile\"]\n",
    "                    location, entity = annotation_dictionary.get(key)\n",
    "    \n",
    "                    ner, startIndex, endIndex = location.split(\" \")\n",
    "                    if \"..\" in entity:\n",
    "                        entity = re.sub(r'([^ \\wõäöüÕÄÖÜ\\.])', r' \\1 ', entity).strip()\n",
    "                        entity = re.sub(r'(\\.{2,})', r' \\1', entity)\n",
    "                    else:\n",
    "                        if entity in no_split_at_tick:\n",
    "                            entity = re.sub(r'([^ \\wõäöüÕÄÖÜ\\'`´-])', r' \\1 ', entity).strip()\n",
    "                        else:\n",
    "                            entity = re.sub(r'([^ \\wõäöüÕÄÖÜ-])', r' \\1 ', entity).strip()\n",
    "                    #entity = re.sub(r\"(')\", r' \\1 ', entity).strip()\n",
    "                    \n",
    "                    for i in range(len(text.words)):\n",
    "                        if text.words[i].start == (int(startIndex) - text.text[:int(text.words[i].start)].count(\"\\n\")):  \n",
    "                            \n",
    "                            preceding_newlines = text.text[:int(text.words[i].start)].count(\"\\n\")\n",
    "                            startIndex = int(startIndex) - int(preceding_newlines)\n",
    "                            endIndex = int(endIndex) - int(preceding_newlines)\n",
    "                            \n",
    "                            if text.words[i].start == startIndex:\n",
    "                                if text.words[i].end == endIndex:\n",
    "                                    base_span = EnvelopingBaseSpan([text.words[i].base_span])\n",
    "                                    name = [text.words[i]]                                    \n",
    "                                else:\n",
    "                                    if text.words[i+1].end == endIndex: \n",
    "                                        name = [text.words[i], text.words[i+1]]\n",
    "                                    else:\n",
    "                                        entity = re.sub(r'\\s\\s+', r' ', entity).split(\" \")\n",
    "                                        if re.match(r'([A-ZÕÄÖÜ]{1}\\w+)[A-ZÕÄÖÜ]{1}\\w+', entity[0]):\n",
    "                                            entity.append(\"\")\n",
    "                                        for j in range(len(entity)):\n",
    "                                            if entity == [\"J\", \".\", \"Laan\"]:\n",
    "                                                name.append(text.words[i+j])\n",
    "                                                break\n",
    "                                            #print(file, j, entity, len(entity), text.words[i].text, text.words[i+j].text)\n",
    "                                            name.append(text.words[i+j])\n",
    "\n",
    "                                base_span = EnvelopingBaseSpan([s.base_span for s in name])\n",
    "                                new_span = EnvelopingSpan(base_span, layer=gold_ner_layer)\n",
    "                                \n",
    "                                if ner == \"Isik\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"PER\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-PER\"\n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-PER\"\n",
    "                                if ner == \"KO_koht\" or ner == \"KO_org\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"LOC_ORG\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-LOC_ORG\"\n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-LOC_ORG\"\n",
    "                                if ner == \"Koht\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"LOC\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-LOC\"\n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-LOC\"\n",
    "                                if ner == \"Org\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"ORG\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-ORG\" \n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-ORG\"\n",
    "                                if ner == \"Muu\" or ner == \"Teadmata\" or ner == \"ese\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"MISC\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-MISC\" \n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-MISC\"\n",
    "                                gold_ner_layer.add_span(new_span)\n",
    "                            break\n",
    "                text.add_layer(gold_ner_layer)\n",
    "                \n",
    "                set1 = list()\n",
    "                set2 = list()\n",
    "                for TUPLE in fixed_annotations:\n",
    "                    set1.append(TUPLE[3])\n",
    "                for NER in text.gold_ner:\n",
    "                    set2.append(NER.enclosing_text)\n",
    "                    \n",
    "                if Diff(set1, set2):\n",
    "                    print(f\"{rownr}. Probleem failis {file}, mille erinevus on: {Diff(set1, set2)}\")\n",
    "                    rownr += 1\n",
    "                \n",
    "                for i in range(0, len(text.words)):\n",
    "                    for key in dictionary_for_wordner.keys():\n",
    "                        new_span = Span(base_span=text.words[i].base_span, layer=gold_wordner_layer)\n",
    "                        if i == key:\n",
    "                            new_span.add_annotation(Annotation(new_span, nertag=str(dictionary_for_wordner.get(key))))\n",
    "                            gold_wordner_layer.add_span(new_span)\n",
    "                            break\n",
    "                        else:\n",
    "                            if i in dictionary_for_wordner.keys():\n",
    "                                continue\n",
    "                            else:\n",
    "                                new_span.add_annotation(Annotation(new_span, nertag=\"O\"))\n",
    "                        gold_wordner_layer.add_span(new_span)\n",
    "                        break\n",
    "                \n",
    "                text.add_layer(gold_wordner_layer)\n",
    "                text_to_json(text, file=cwd + \"/vallakohtufailid_json/\" + file.replace(\".txt\", \".json\"))\n",
    "    print(f\"Kaust {directory} on läbitud.\")\n",
    "print(\"Programm on lõpetanud oma töö.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

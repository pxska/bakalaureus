{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from modules.preprocessing_protocols import preprocess_text\n",
    "from estnltk import EnvelopingBaseSpan\n",
    "from estnltk import Text, Layer, Annotation, EnvelopingSpan, Span\n",
    "from estnltk.converters import text_to_json\n",
    "from estnltk.layer_operations import extract_sections\n",
    "from estnltk.layer_operations import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Probleem failis Viljandi_Paistu_Holstre_id9042_1836a.txt, mille erinevus on ['Jaan Park_']\n",
      "2. Probleem failis Tartu_Kodavere_Pala_id22870_1872a.txt, mille erinevus on ['Jaan\\nAnni']\n",
      "Kaust vallakohus_esimene/ on läbitud.\n",
      "Kaust vallakohus_teine/ on läbitud.\n",
      "Kaust vallakohus_kolmas/ on läbitud.\n",
      "3. Probleem failis Tartu_Laiuse_Kivij2rve_id13164_1866a.txt, mille erinevus on ['Thomas Peterson']\n",
      "Kaust vallakohus_neljas/ on läbitud.\n",
      "Programm on lõpetanud oma töö.\n"
     ]
    }
   ],
   "source": [
    "# If the annotation contains newline character, then indexes will contain ';' at the linebreak (e.g. 388 393;394 398 )\n",
    "indexes_on_line_split = re.compile(r' (\\d+) (\\d+;\\d+ ){1,}(\\d+)$')\n",
    "\n",
    "def collect_annotations( in_f ):\n",
    "    annotations = []\n",
    "    split_lines_ahead = 0\n",
    "    for line in in_f:\n",
    "        line = line.rstrip('\\n')\n",
    "        items = line.split('\\t')\n",
    "        if split_lines_ahead > 0:\n",
    "            split_lines_ahead -= 1\n",
    "            last_item = annotations[-1]\n",
    "            new_tuple = (last_item[0],last_item[1],last_item[2],(last_item[3]+line),last_item[4])\n",
    "            annotations[-1] = new_tuple\n",
    "            continue\n",
    "        if len(items) == 3:\n",
    "            indexes_str = items[1]\n",
    "            if indexes_str.count(';') > 0:\n",
    "                split_lines_ahead += indexes_str.count(';')\n",
    "            indexes_str = indexes_on_line_split.sub(' \\\\1 \\\\3', indexes_str)\n",
    "            tag, start, end = indexes_str.split()\n",
    "            annotations.append( (tag, start, end, items[2], items[0]) )\n",
    "    seen = set()\n",
    "    removed_duplicates_annotations = []\n",
    "    for a, b, c, d, e in annotations:\n",
    "        if not b in seen:\n",
    "            seen.add(b)\n",
    "            removed_duplicates_annotations.append((a, b, c, d, e))\n",
    "        else:\n",
    "            for index, item in enumerate(removed_duplicates_annotations):\n",
    "                if item[1] == b and item[2] > c:\n",
    "                    tuple_without_n = (a, b, c, d, e)\n",
    "                    item = tuple_without_n\n",
    "                    removed_duplicates_annotations[index] = item\n",
    "                elif item[1] == b and item[2] < c:\n",
    "                    tuple_without_n = (a, b, item[2], d, e)\n",
    "                    item = tuple_without_n\n",
    "                    removed_duplicates_annotations[index] = item\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "    for index, item in enumerate(removed_duplicates_annotations):\n",
    "        if \"\\xa0\" in item[3]:\n",
    "            replaced = re.sub(r'\\s\\s+', r' ', item[3].replace(u'\\xa0', u' '))\n",
    "            removed_duplicates_annotations[index] = ( item[0], item[1], item[2], replaced, item[4] )\n",
    "    \n",
    "    annotations = sorted(list(set(removed_duplicates_annotations)), key=lambda x: int(x[1]))\n",
    "    return annotations\n",
    "\n",
    "rownr = 1\n",
    "directories = [\"vallakohus_esimene/\", \"vallakohus_teine/\", \"vallakohus_kolmas/\", \"vallakohus_neljas/\"]\n",
    "for directory in directories:\n",
    "    path = \"./vallakohtufailid/\" + directory\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(path + file, 'r', encoding=\"UTF-8\") as txt, \\\n",
    "                 open(path + file.split(\".\")[0] + \".ann\", 'r', encoding=\"UTF-8\") as ann:\n",
    "                textfile = txt.read().replace(u'\\xa0', ' ')\n",
    "                \n",
    "                if file == \"Tartu_V6nnu_Ahja_id3502_1882a.txt\":\n",
    "                    textfile = textfile.replace('..', '. .')\n",
    "                \n",
    "                dictionary_for_wordner = dict()\n",
    "                \n",
    "                # Converting the text form .txt file into an EstNLTK Text object and giving it the \"morph_analysis\" layer\n",
    "                text = Text(textfile)\n",
    "                text.meta['origin_directory'] = str(directory)\n",
    "                text = preprocess_text(text)\n",
    "                \n",
    "                # Creating NER layers\n",
    "                gold_ner_layer = Layer(name=\"unflattened_gold_ner\", text_object=text, attributes=['nertag'])\n",
    "                gold_wordner_layer = Layer(name=\"unflattened_gold_wordner\", text_object=text, attributes=['nertag'], parent=\"words\")\n",
    "                \n",
    "                # Fixing annotations (some annotations are on different rows)\n",
    "                fixed_annotations = collect_annotations(ann)\n",
    "\n",
    "                # Collecting the annotations in a separate dictionary\n",
    "                annotation_dictionary = {}\n",
    "                for annotation in fixed_annotations:\n",
    "                    trigger = annotation[4]\n",
    "                    location = annotation[0] + \" \" + annotation[1] + \" \" + annotation[2]\n",
    "                    entity = annotation[3]\n",
    "                    annotation_dictionary[trigger] = [location, entity]\n",
    "\n",
    "                # Iterating through the keys (triggers) of the dictionary\n",
    "                for key in annotation_dictionary:\n",
    "                    name = []\n",
    "                    \n",
    "                    location, entity = annotation_dictionary.get(key)\n",
    "    \n",
    "                    ner, startIndex, endIndex = location.split(\" \")\n",
    "      \n",
    "                    for i in range(len(text.words)):\n",
    "                        if text.words[i].start == (int(startIndex) - text.text[:int(text.words[i].start)].count(\"\\n\")):  \n",
    "                            \n",
    "                            preceding_newlines = text.text[:int(text.words[i].start)].count(\"\\n\")\n",
    "                            startIndex = int(startIndex) - int(preceding_newlines)\n",
    "                            endIndex = int(endIndex) - int(preceding_newlines)\n",
    "                            \n",
    "                            # Exceptions in some files\n",
    "                            if entity == \"Gustav  Waddi\" or (text.words[i].text == entity[:-1] and entity[-1] == \" \"):\n",
    "                                endIndex -= 1\n",
    "                            if text.words[i] == \"..\":\n",
    "                                endindex -= 2\n",
    "                            if \"\\n\" in text.text[startIndex:endIndex] and entity != \"Gustav  Waddi\":\n",
    "                                endIndex -= text.text[startIndex:endIndex].count(\"\\n\")\n",
    "                            if entity == \"Jaan Park\" and text.words[i+1].text == \"Park_\":\n",
    "                                endIndex += 1\n",
    "                            \n",
    "                            # Creating a base span based on the start index and end index.\n",
    "                            if text.words[i].start == startIndex:\n",
    "                                if text.words[i].end == endIndex:\n",
    "                                    base_span = EnvelopingBaseSpan([text.words[i].base_span])\n",
    "                                    name = [text.words[i]]                                    \n",
    "                                else:\n",
    "                                    if text.words[i+1].end == endIndex: \n",
    "                                        name = [text.words[i], text.words[i+1]]\n",
    "                                    else:\n",
    "                                        iterator = 0\n",
    "                                        while True:\n",
    "                                            if text.words[i+iterator].end == endIndex:\n",
    "                                                name.append(text.words[i+iterator])\n",
    "                                                iterator = 0\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                name.append(text.words[i+iterator])\n",
    "                                                iterator += 1\n",
    "                                \n",
    "                                base_span = EnvelopingBaseSpan([s.base_span for s in name])\n",
    "                                new_span = EnvelopingSpan(base_span, layer=gold_ner_layer)\n",
    "                                \n",
    "                                # Creating named entities based on aforementioned spans\n",
    "                                if ner == \"Isik\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"PER\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-PER\"\n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-PER\"\n",
    "                                if ner == \"KO_koht\" or ner == \"KO_org\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"LOC_ORG\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-LOC_ORG\"\n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-LOC_ORG\"\n",
    "                                if ner == \"Koht\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"LOC\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-LOC\"\n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-LOC\"\n",
    "                                if ner == \"Org\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"ORG\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-ORG\" \n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-ORG\"\n",
    "                                if ner == \"Muu\" or ner == \"Teadmata\" or ner == \"ese\":\n",
    "                                    new_span.add_annotation(Annotation(new_span, nertag=\"MISC\"))\n",
    "                                    for k in range(0, len(name)):\n",
    "                                        if k == 0:\n",
    "                                            dictionary_for_wordner[i] = \"B-MISC\" \n",
    "                                        else:\n",
    "                                            dictionary_for_wordner[i+k] = \"I-MISC\"\n",
    "                                gold_ner_layer.add_span(new_span)\n",
    "                            break\n",
    "                text.add_layer(gold_ner_layer)\n",
    "                \n",
    "                # Finding the difference between annotations in the file and annotations on the text\n",
    "                set1 = [re.sub(r'\\s\\s+', r' ', element[3].strip()) for element in fixed_annotations]\n",
    "                set2 = [re.sub(r'\\s\\s+', r' ', element.enclosing_text.strip()) for element in text.unflattened_gold_ner]\n",
    "                diff = list(list(set(set1)-set(set2)) + list(set(set2)-set(set1)))\n",
    "                if (diff):\n",
    "                    print(f\"{rownr}. Probleem failis {file}, mille erinevus on {diff}\")\n",
    "                    rownr += 1\n",
    "                \n",
    "                # Creating WordNer annotations:\n",
    "                for i in range(0, len(text.words)):\n",
    "                    for key in dictionary_for_wordner.keys():\n",
    "                        new_span = Span(base_span=text.words[i].base_span, layer=gold_wordner_layer)\n",
    "                        if i == key:\n",
    "                            new_span.add_annotation(Annotation(new_span, nertag=str(dictionary_for_wordner.get(key))))\n",
    "                            gold_wordner_layer.add_span(new_span)\n",
    "                            break\n",
    "                        else:\n",
    "                            if i in dictionary_for_wordner.keys():\n",
    "                                continue\n",
    "                            else:\n",
    "                                new_span.add_annotation(Annotation(new_span, nertag=\"O\"))\n",
    "                        gold_wordner_layer.add_span(new_span)\n",
    "                        break\n",
    "                \n",
    "                text.add_layer(gold_wordner_layer)\n",
    "                \n",
    "                # Flattening the layers to save up on space.\n",
    "                text.add_layer(flatten(text['unflattened_gold_ner'], 'gold_ner'))\n",
    "                text.add_layer(flatten(text['unflattened_gold_wordner'], 'gold_wordner'))\n",
    "                text.pop_layer('words')\n",
    "                text.pop_layer('tokens')\n",
    "                text.pop_layer('unflattened_gold_ner')\n",
    "                text.gold_wordner.ambiguous = False\n",
    "                text.gold_ner.ambiguous = False\n",
    "                # Saving the new json files to a separate folder.\n",
    "                text_to_json(text, file=\"./vallakohtufailid-json-flattened/\" + file.replace(\".txt\", \".json\"))\n",
    "    print(f\"Kaust {directory} on läbitud.\")\n",
    "print(\"Programm on lõpetanud oma töö.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

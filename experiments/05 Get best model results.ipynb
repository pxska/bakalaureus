{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "from modules.preprocessing_protocols import preprocess_text\n",
    "from modules.results_extraction import extract_results, results_by_subdistribution,\\\n",
    "                                    results_by_named_entity, confusion_matrix\n",
    "from modules.tools import find\n",
    "\n",
    "from estnltk import Text\n",
    "from estnltk.taggers import NerTagger\n",
    "from estnltk.converters import text_to_json\n",
    "from estnltk.converters import json_to_text\n",
    "from estnltk.layer_operations import flatten\n",
    "\n",
    "from estnltk.taggers.estner.ner_trainer import NerTrainer\n",
    "from estnltk.taggers.estner.model_storage_util import ModelStorageUtil\n",
    "\n",
    "from estnltk.taggers import VabamorfCorpusTagger\n",
    "vm_corpus_tagger = VabamorfCorpusTagger()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nervaluate import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "divided_corpus = os.path.join('..', 'data', 'divided_corpus.txt')\n",
    "json_files_location = os.path.join('..', 'data', 'vallakohtufailid-json-flattened')\n",
    "vallakohtufailid_location = os.path.join('..', 'data', 'vallakohtufailid')\n",
    "no_goldstandard_tags_location = os.path.join('..', 'data', 'files_without_goldstandard_annotations.txt')\n",
    "testing_files_location = json_files_location\n",
    "testing_files_names = os.path.join('..', 'data', 'corpus_subdistribution_without_hand_tagged.txt')\n",
    "\n",
    "removed_layers = ['sentences', 'morph_analysis', 'compound_tokens', 'ner', 'words', 'tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get files without gold-standard annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(no_goldstandard_tags_location, 'r', encoding='UTF-8') as in_f:\n",
    "    lines = in_f.readlines()\n",
    "\n",
    "no_goldstandard_annotations = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all files (subdistributions 1â€“5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "\n",
    "with open(os.path.join(divided_corpus), 'r', encoding = 'UTF-8') as f:\n",
    "    txt = f.readlines()\n",
    "\n",
    "for fileName in txt:\n",
    "    file, subdistribution = fileName.split(\":\")\n",
    "    files[file] = subdistribution.rstrip(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get files for testing (6th subdistribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_files = {}\n",
    "\n",
    "with open(os.path.join(testing_files_names), 'r', encoding = 'UTF-8') as in_f:\n",
    "    txt = in_f.readlines()\n",
    "\n",
    "for filename in txt:\n",
    "    file, subdistribution = filename.split(\":\")\n",
    "    testing_files[file] = subdistribution.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_texts(filenames):\n",
    "    print(\"(!) Preparing training texts\")\n",
    "    training_texts = []\n",
    "    \n",
    "    for file in filenames:\n",
    "        if file in no_goldstandard_annotations:\n",
    "            continue\n",
    "        else:\n",
    "            with open(os.path.join(json_files_location, file), 'r', encoding='UTF-8') as in_f:\n",
    "                tagged_text = preprocess_text(json_to_text(in_f.read()))\n",
    "                training_texts.append(tagged_text)\n",
    "    print(f\"(!) Training texts prepared\")\n",
    "    return training_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nertagger(training_texts, new_model_dir):\n",
    "    print(\"(!) Training NerTagger\")    \n",
    "    modelUtil = ModelStorageUtil( new_model_dir )\n",
    "    nersettings = modelUtil.load_settings()\n",
    "    trainer = NerTrainer(nersettings)\n",
    "    trainer.train( training_texts, layer='gold_wordner', model_dir=new_model_dir )\n",
    "    print(f\"(!) NerTagger trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_files(model_dir, testing_files):\n",
    "    print(\"(!) Tagging files\")\n",
    "\n",
    "    nertagger = NerTagger(model_dir)\n",
    "    iterator = 1\n",
    "\n",
    "    for test_file in testing_files:\n",
    "        with open(find(test_file.replace('.json', '.txt'), vallakohtufailid_location), 'r', encoding='UTF-8') as in_f:\n",
    "            text = in_f.read()\n",
    "        \n",
    "        if test_file == \"Tartu_V6nnu_Ahja_id3502_1882a.json\":\n",
    "            text = text.replace('..', '. .')\n",
    "        text = preprocess_text(Text(text))\n",
    "\n",
    "        nertagger.tag(text)\n",
    "        text.add_layer(flatten(text['ner'], 'flat_ner'))\n",
    "\n",
    "        for x in removed_layers:\n",
    "            text.pop_layer(x)\n",
    "\n",
    "        path = os.path.join(model_dir, 'vallakohtufailid-trained-nertagger')\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "            \n",
    "        text_to_json(text, file=os.path.join(model_dir, 'vallakohtufailid-trained-nertagger', test_file))\n",
    "        \n",
    "        print(f'{iterator}. Tagged file {test_file}')\n",
    "        iterator += 1\n",
    "    \n",
    "    print(f\"(!) All files tagged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_directory):\n",
    "\n",
    "    # Get the filenames to be trained on from the files dictionary\n",
    "    filenames = [key for key, value in files.items()]\n",
    "\n",
    "    # Create training_texts from the aforementioned filenames\n",
    "    training_texts = create_training_texts(filenames)\n",
    "\n",
    "    # Set up the trainer and training\n",
    "    new_model_dir = os.path.join('models', model_directory)\n",
    "    train_nertagger(training_texts, new_model_dir)\n",
    "\n",
    "    # Set up the new trained nertagger\n",
    "    tagger = NerTagger(model_dir = new_model_dir)\n",
    "\n",
    "    # Tag the files using the new nertagger\n",
    "    tag_files(new_model_dir, testing_files)\n",
    "            \n",
    "    # Get results of model\n",
    "    extract_results(model_directory, #model directory path\n",
    "                    testing_files,\n",
    "                    no_goldstandard_annotations,\n",
    "                    os.path.join('models', model_directory, 'vallakohtufailid-trained-nertagger'), #files tagged by trained nertagger location\n",
    "                    testing_files_location,\n",
    "                    os.path.join('models', model_directory)) #results.txt location\n",
    "    \n",
    "    print(f\"(!) Model {model_directory} trained & tested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = os.path.join('model_morph_with_lemmas_and_sentences_and_gazetteer_and_global_features', 'model_gaz_loc_variants_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Precision    Recall        F1\n",
      "0   0.924694  0.892192  0.908152\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('models', model_directory, 'results.txt'), 'r', encoding='UTF-8') as in_f:\n",
    "    print(results_by_subdistribution(json.loads(in_f.read()), testing_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          6     Total\n",
      "ORG_precision      0.838235  0.838235\n",
      "ORG_recall         0.780822  0.780822\n",
      "ORG_f1score        0.808511  0.808511\n",
      "PER_precision      0.953627  0.953627\n",
      "PER_recall         0.936574  0.936574\n",
      "PER_f1score        0.945024  0.945024\n",
      "MISC_precision     0.684211  0.684211\n",
      "MISC_recall        0.577778  0.577778\n",
      "MISC_f1score       0.626506  0.626506\n",
      "LOC_precision      0.601695  0.601695\n",
      "LOC_recall         0.489655  0.489655\n",
      "LOC_f1score        0.539924  0.539924\n",
      "LOC_ORG_precision  0.786325  0.786325\n",
      "LOC_ORG_recall     0.695214  0.695214\n",
      "LOC_ORG_f1score    0.737968  0.737968\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('models', model_directory, 'results.txt'), 'r', encoding='UTF-8') as in_f:\n",
    "    print(results_by_named_entity(json.loads(in_f.read()), testing_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  LOC  LOC_ORG  MISC  ORG   PER\n",
      "Actual                                  \n",
      "LOC         71       10     0    1    12\n",
      "LOC_ORG      5      276     0    0    11\n",
      "MISC         0        0    26    0     5\n",
      "ORG          0        1     0   57     1\n",
      "PER          2        3     0    0  3352\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = confusion_matrix(model_directory, testing_files, no_goldstandard_annotations, os.path.join('models', model_directory, 'vallakohtufailid-trained-nertagger'), testing_files_location)\n",
    "\n",
    "print(pd.crosstab(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

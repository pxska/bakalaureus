{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert BRAT-tool annotated files to \"goldstandard\" .json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from modules.preprocessing_protocols import preprocess_text\n",
    "from modules.collect_annotations import collect_annotations\n",
    "from modules.get_layer_difference import find_difference\n",
    "\n",
    "from estnltk import EnvelopingBaseSpan\n",
    "from estnltk import Text, Layer, Annotation, EnvelopingSpan, Span\n",
    "from estnltk.converters import text_to_json\n",
    "from estnltk.layer_operations import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define directories in which the brat-annotated files (\\*.txt and \\*.ann) are located. For example, <code>vallakohus_esimene</code> and <code>vallakohus_teine</code> contain file pairs:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\"vallakohus_esimene\", \"vallakohus_teine\", \"vallakohus_kolmas\", \"vallakohus_neljas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define the location of the aforementioned directories in relation to the Jupyter Notebook. Also define the location where the .json files will be saved to:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vallakohtufailid_location = os.path.join('..', 'data', 'vallakohtufailid')\n",
    "json_files_location = os.path.join('..', 'data', 'vallakohtufailid-json-flattened')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define named entities in the BRAT-tool annotated files:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities = {\n",
    "    \"Isik\": \"PER\",\n",
    "    \"KO_koht\": \"LOC_ORG\",\n",
    "    \"KO_org\": \"LOC_ORG\",\n",
    "    \"Koht\": \"LOC\",\n",
    "    \"Org\": \"ORG\",\n",
    "    \"Muu\": \"MISC\",\n",
    "    \"Teadmata\": \"MISC\",\n",
    "    \"ese\": \"MISC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(!) Difference in Viljandi_Paistu_Holstre_id9042_1836a.txt correct annotations and output annotations: ['Jaan Park_']\n",
      "(!) Difference in Tartu_Kodavere_Pala_id22870_1872a.txt correct annotations and output annotations: ['Jaan\\nAnni']\n",
      "(!) Directory vallakohus_esimene is completed\n",
      "(!) Directory vallakohus_teine is completed\n",
      "(!) Directory vallakohus_kolmas is completed\n",
      "(!) Difference in Tartu_Laiuse_Kivij2rve_id13164_1866a.txt correct annotations and output annotations: ['Thomas Peterson']\n",
      "(!) Directory vallakohus_neljas is completed\n",
      "(!) The code has finished!\n"
     ]
    }
   ],
   "source": [
    "for directory in directories:\n",
    "    path = os.path.join(vallakohtufailid_location, directory)\n",
    "    files = [filename for filename in os.listdir(path) if filename.endswith('.txt')]\n",
    "    for file in files:\n",
    "        with open(os.path.join(path, file), 'r', encoding=\"UTF-8\") as in_txt:\n",
    "            in_txt = in_txt.read().replace(u'\\xa0', ' ')\n",
    "\n",
    "        if file == \"Tartu_V6nnu_Ahja_id3502_1882a.txt\":\n",
    "            in_txt = in_txt.replace('..', '. .')\n",
    "\n",
    "        dictionary_for_wordner = dict()\n",
    "\n",
    "        # Convert text into EstNLTK Text object and preprocess it\n",
    "        text = Text(in_txt)\n",
    "        text.meta['origin_directory'] = str(directory)\n",
    "        preprocess_text(text)\n",
    "\n",
    "        # Create NER layers\n",
    "        gold_ner_layer = Layer(name=\"unflattened_gold_ner\", text_object=text, attributes=['nertag'])\n",
    "        gold_wordner_layer = Layer(name=\"unflattened_gold_wordner\", text_object=text, attributes=['nertag'], parent=\"words\")\n",
    "\n",
    "        # Fix annotations\n",
    "        with open(os.path.join(path, file.split(\".\")[0] + \".ann\"), 'r', encoding=\"UTF-8\") as in_ann:\n",
    "            fixed_annotations = collect_annotations(in_ann)\n",
    "\n",
    "        # Collect the annotations in a separate dictionary\n",
    "        annotation_dictionary = {}\n",
    "        for annotation in fixed_annotations:\n",
    "            trigger = annotation[4]\n",
    "            location = annotation[0] + \" \" + annotation[1] + \" \" + annotation[2]\n",
    "            entity = annotation[3]\n",
    "            annotation_dictionary[trigger] = [location, entity]\n",
    "\n",
    "        # Iterate through the keys (triggers) of the dictionary\n",
    "        for key in annotation_dictionary:\n",
    "            name = []\n",
    "\n",
    "            location, entity = annotation_dictionary.get(key)\n",
    "\n",
    "            ner, startIndex, endIndex = location.split(\" \")\n",
    "\n",
    "            for i in range(len(text.words)):\n",
    "                if text.words[i].start == (int(startIndex) - text.text[:int(text.words[i].start)].count(\"\\n\")):  \n",
    "\n",
    "                    preceding_newlines = text.text[:int(text.words[i].start)].count(\"\\n\")\n",
    "                    startIndex = int(startIndex) - int(preceding_newlines)\n",
    "                    endIndex = int(endIndex) - int(preceding_newlines)\n",
    "\n",
    "                    # NB! Exceptions in some of the files:\n",
    "                    if entity == \"Gustav  Waddi\" or (text.words[i].text == entity[:-1] and entity[-1] == \" \"):\n",
    "                        endIndex -= 1\n",
    "                    if text.words[i] == \"..\":\n",
    "                        endindex -= 2\n",
    "                    if \"\\n\" in text.text[startIndex:endIndex] and entity != \"Gustav  Waddi\":\n",
    "                        endIndex -= text.text[startIndex:endIndex].count(\"\\n\")\n",
    "                    if entity == \"Jaan Park\" and text.words[i+1].text == \"Park_\":\n",
    "                        endIndex += 1\n",
    "\n",
    "                    # Create a base span based on the start index and end index\n",
    "                    if text.words[i].start == startIndex:\n",
    "                        if text.words[i].end == endIndex:\n",
    "                            base_span = EnvelopingBaseSpan([text.words[i].base_span])\n",
    "                            name = [text.words[i]]                                    \n",
    "                        else:\n",
    "                            if text.words[i+1].end == endIndex: \n",
    "                                name = [text.words[i], text.words[i+1]]\n",
    "                            else:\n",
    "                                iterator = 0\n",
    "                                while True:\n",
    "                                    if text.words[i+iterator].end == endIndex:\n",
    "                                        name.append(text.words[i+iterator])\n",
    "                                        iterator = 0\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        name.append(text.words[i+iterator])\n",
    "                                        iterator += 1\n",
    "\n",
    "                        base_span = EnvelopingBaseSpan([s.base_span for s in name])\n",
    "                        new_span = EnvelopingSpan(base_span, layer=gold_ner_layer)\n",
    "\n",
    "                        # Create named entities based on aforementioned spans\n",
    "                        new_span.add_annotation(Annotation(new_span, nertag=named_entities[ner]))\n",
    "                        for k in range(0, len(name)):\n",
    "                            if k == 0:\n",
    "                                dictionary_for_wordner[i] = f'B-{named_entities[ner]}'\n",
    "                            else:\n",
    "                                dictionary_for_wordner[i+k] = f'I-{named_entities[ner]}'\n",
    "\n",
    "                        gold_ner_layer.add_span(new_span)\n",
    "                        \n",
    "                    break\n",
    "        text.add_layer(gold_ner_layer)\n",
    "\n",
    "        # Find the difference between annotations in the file and annotations on the text\n",
    "        find_difference(file, fixed_annotations, text.unflattened_gold_ner)\n",
    "\n",
    "        # Create wordner annotations\n",
    "        for i in range(0, len(text.words)):\n",
    "            for key in dictionary_for_wordner.keys():\n",
    "                new_span = Span(base_span=text.words[i].base_span, layer=gold_wordner_layer)\n",
    "                if i == key:\n",
    "                    new_span.add_annotation(Annotation(new_span, nertag=str(dictionary_for_wordner.get(key))))\n",
    "                    gold_wordner_layer.add_span(new_span)\n",
    "                    break\n",
    "                else:\n",
    "                    if i in dictionary_for_wordner.keys():\n",
    "                        continue\n",
    "                    else:\n",
    "                        new_span.add_annotation(Annotation(new_span, nertag=\"O\"))\n",
    "                gold_wordner_layer.add_span(new_span)\n",
    "                break\n",
    "\n",
    "        text.add_layer(gold_wordner_layer)\n",
    "\n",
    "        # Flatten the layers and remove extra layers to save space\n",
    "        remove_layers = ['words', 'tokens', 'unflattened_gold_ner']\n",
    "\n",
    "        text.add_layer(flatten(text['unflattened_gold_ner'], 'gold_ner'))\n",
    "        text.add_layer(flatten(text['unflattened_gold_wordner'], 'gold_wordner'))\n",
    "\n",
    "        for layer in remove_layers:\n",
    "            text.pop_layer(layer)\n",
    "\n",
    "        text.gold_wordner.ambiguous = False\n",
    "        text.gold_ner.ambiguous = False\n",
    "\n",
    "        # Save the new json files to a separate directory\n",
    "        if not os.path.exists(json_files_location):\n",
    "            os.mkdir(json_files_location)\n",
    "\n",
    "        text_to_json(text, file=os.path.join(json_files_location, file.replace(\".txt\", \".json\")))\n",
    "        \n",
    "    print(f\"(!) Directory {directory} is completed\")\n",
    "print(\"(!) The code has finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
